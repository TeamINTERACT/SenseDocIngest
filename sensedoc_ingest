#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

NOTE:
This code has been instrumented to run multithreaded over the I/O
sections, but to make use of that, we need to run the job wrapped in
a bash shell that sets up the SLURM settings. Otherwise no additional
threads or processors will be available in the runtime environment.

Usage:
  sensedoc_ingest [options] PATH
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -i IID        Ingest participant IID only
    -v,--verbose  Provide more verbose output
"""
import os
import sqlite3
import datetime
import subprocess as sub
from docopt import docopt
from itertools import combinations
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor as PoolExecutor

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 3600 # equates to 1 hour of usable data
dbname = 'interact_db'
logfile = datetime.datetime.now().strftime("ingest_%Y%m%d-%H%M%S.log")
loghandle = None

# number of processing threads to run
num_threads = 1

def log(msg):
    if loghandle:
        loghandle.write("%s\n" % msg)
    print("LOG: %s" % msg)

def mention(str):
    if args['--verbose']:
        print(str)
        if loghandle:
            loghandle.write("%s\n"%str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    """
    Given the start and end of two different date ranges, determine
    whether the ranges overlap.
    """
    # Make sure each start and end pair are in sorted order
    start1 = min(dt1_st, dt1_end)
    end1 = max(dt1_st, dt1_end)
    start2 = min(dt2_st, dt2_end)
    end2 = max(dt2_st, dt2_end)

    # If one range occurs entirely before the other, there is no overlap.
    # Otherwise there is.
    return not (end1 < start2 or start1 > end2)

def select_usable_files(filelist, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the set contains acceptable data, return a list of them. 
    Otherwise, return None.

    The entire set should be considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So to speed up the process, I'm assuming that if the gps table 
    # contains good dates and data, the accel table will as well. 
    sql = """
            SELECT MIN(utcdate) as mindate,
                   MAX(utcdate) as maxdate,
                   COUNT(*) as count
            FROM gps
            """
            # It turns out that the following 3 queries are MUCH
            # faster than the one above, executing in about 1/8th 
            # the time. And since there's a non-trivial amount of 
            # time spent doing this, it's worth optimizing.
    sql_min = "SELECT utcdate from gps order by ts ASC limit 1"
    sql_max = "SELECT utcdate from gps order by ts DESC limit 1"
    sql_n   = "SELECT count(1) as n from gps"

    fstats = []
    good_files = []
    for fpath in filelist:
        # open the file and compute its date range and record count
        try:
            with sqlite3.connect(fpath) as conn:
                c = conn.cursor()
                c.execute(sql_min)
                mindate = c.fetchone()[0]
                c.execute(sql_max)
                maxdate = c.fetchone()[0]
                c.execute(sql_n)
                count = c.fetchone()[0]
        except Exception as e:
            log("Caught a SQL error while counting records.")
            log(e)

        # make a note of those stats if they're usable
        if mindate and maxdate and count:
            fstats.append([mindate,maxdate,count,fpath])
            good_files.append(fpath)
        else: # otherwise leave this file out of the list
            log("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                        mindate,
                                                        maxdate,
                                                        count))

    if len(fstats) > 1:
        # compare all possible pairings of files and reject 
        # this filelist if any pair has overlap between 
        # their min and max date stamps.
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                spanstr = "   %s - %s\n   %s - %s"%(stat1[0], stat1[1], 
                                                stat2[0], stat2[1])
                log("Overlapping timestamps found between:\n  %s\n  %s" % (stat1[3],stat2[3]))
                return None
        # total the number of samples from all files and
        # reject the filelist if the number of samples does not
        # meet the minimum threshold
        if sum([x[2] for x in fstats]) < min_sample_threshold:
            log("Not enough samples for participant %s"%iidstr)
            return None
        return good_files
    elif len(fstats) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return good_files

    log("Insufficient usable data for participant %s." % iidstr)
    return None

def execute_copy_via_shell(argset):
    # The easiest way to invoke this func from the thread pool manager
    # is to pack up the arguments for each call into a list. So first
    # we have to unpack the function arguments.
    [filepath, tablename, selectionsql, pid] = argset

    # Now we have everything we need to ingest the given datafile. 
    # On the command line we could run a single command to
    # pull data from the sqlite file with sqlite3 and ingest into the 
    # postgres by piping the output directly to psql.
    #
    # In simple form, that console command would be:
    # sqlite3 somefile.sdb SELECTIONSQL | psql interact_db INSERTSQL
    #
    # The prototype did all of this work in a series of Python Pandas 
    # scripts which made multiple passes of the datafiles to merge them
    # clean them, and then push them into the DB. But the resulting 
    # process was horribly slow. 
    #
    # This time, we're going to do it in a single pass of the data, and
    # we'll employ the sqlite3 and psql clients to do the heavy lifting. 
    # These tools have both been optimized for CSV streaming, and can 
    # load the data into the DB much more efficiently.
    #
    # In the simple form of the command line shown above, the two
    # key elements are SELECTIONSQL, which is the SQL fragment used to
    # select a table's content out of the target SQLite file, and the
    # INSERTSQL, which is the SQL fragment used to load the incoming
    # data into the PostgreSQL database.
    # 
    # In addition to doing the simple select/insert, these commands 
    # will also handle many of the cleaning and normalization steps
    # that need to be applied to the data before it can be used.
    #
    # Unfortunately, the data is known to contain occasional duplicate 
    # records, due to a minor glitch in the capture hardware. So the 
    # choice is to remove those duplicates at SELECT time, when reading
    # them out of the sqlite file, or at INSERT time, when reading them
    # into the database. 
    #
    # Eliminating them sooner would be preferable.
    #    Running a test on a single file with 22 duplicates,
    #    eliminating the duplicates at selection time added 1.1s to the
    #    processing time for that file (1.7s vs 2.8s)
    #    Running a test on duplicate removal at ingest time gave us
    #
    # So, to eliminate duplicates, we'll create a dummy table and load
    # the data into that, and then use INSERT ON CONFLICT DO NOTHING to
    # ignore duplicates as we copy the dummy table to the live table.
    # print("Filepath: %s"%filepath)
    # print("Tablename: %s"%tablename)
    # print("PID: %s"%pid)
    # tableid = '_%s_%s' % (tablename, pid)
    tableid = '_%s_%s' % (filepath[-20:],tablename)
    tableid = tableid.translate({ord(i):None for i in '-./'})
    # tableid = ''
    copycmd = """
             CREATE TABLE level_0.deleteme{tid} (LIKE level_0.{tn}); 
             COPY level_0.deleteme{tid} FROM STDIN delimiter '|' CSV;
             INSERT INTO level_0.{tn}
                SELECT * FROM level_0.deleteme{tid}
                ON CONFLICT (iid,ts) DO NOTHING;
             DROP TABLE level_0.deleteme{tid};
             """.format(tn=tablename,tid=tableid)

    cmdline = 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath,
                                                        selectionsql,
                                                        dbname,
                                                        copycmd)
    mention("Command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If any table fails, the ingest for that participant should be 
        # reported as unsuccessful.
        log("Ingesting file %s failed with return code: %s"%(filepath,res))
        return False
    return True

def ingest_sdb_file(argset):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    [filepath,iidstr] = argset
    refDate = ''
    with sqlite3.connect(filepath) as conn:
        # get the reference date from the SDB from which all 
        # timestamps are measured
        c = conn.cursor()
        c.execute("SELECT value FROM ancillary WHERE key='refDate'")
        refDate = c.fetchone()[0]

    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

    # From the prototype system, the fuse_sd_pdata.py script contains
    # a number of data scrubbing and polishing steps. These same steps
    # need to be implemented in this newer (and hopefully faster) ingest
    # system.
    # X Reject filesets with overlapping time windows (and notify admin)
    # X Add the participant's iid to every sample record
    # X Merge the timestamps into one value with millisec precision
    # X Scale the x,y,z values of the accel table
    # X Set empty string as default value for all text and char fields
    # - All fields must be NOT NULL and have sensible defaults
    # - gps.lat must be between -90 and 90 inclusive
    # - gps.lon must be between -180 and 180 inclusive
    # - gps.sat_in_view to integer and default to -1
    # - Reject records that duplicate existing value of iid,ts
    #      Unfortunately, the records with duplicate timestamps are 
    #      not duplicated in all fields, so we can't just do
    #      a simple SAMPLE DISTINCT when exporting from the SQLite file.
    #      Consequently, the most efficient solution I've found is to
    #      load the file into a dummy DB table and then copy from there 
    #      into the actual table with INSERT ON CONFLICT DO NOTHING.
    #      It requires a second pass of the ingested data, but it's a
    #      pretty efficient second pass.

    # Each table in the SQLite file needs its own SELECT stmt
    # to ensure data is read from the file in a known format.
    selectionsql = {}
    selectionsql['sd_accel'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---convert to 0.0-1.0
                            y * 0.00390625 as y, ---convert to 0.0-1.0
                            z * 0.00390625 as z  ---convert to 0.0-1.0
                       FROM accel
                       WHERE x is not null
                         AND y is not null
                         AND z is not null;
                       """.format(iidstr, refDate)
    selectionsql['sd_gps'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            lat, lon, speed, course, mode, fix, alt, 
                            mode1, mode2, sat_used, 
                            pdop, hdop, vdop, sat_in_view 
                            FROM gps
                            WHERE lat is not null
                              AND lon is not null
                              AND lat BETWEEN -90.0 and 90.0
                              AND lon BETWEEN -180.0 and 180.0
                              AND alt BETWEEN -10000 and 100000
                              AND sat_used >= 3;
                        """.format(iidstr, refDate)
        # NOTE: In the above queries, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of 1000000.0, 
        # to preserve the floating-point nature of the result.

    # In a moment, we're going to ingest the individual files into the 
    # DB, but to do it quickly, we're going to dispatch multiple 
    # threads. In preparation for that step, we're going to
    # pre-construct the set of parameters that will be passed to the
    # actual ingestor function - one set of args for each invocation.
    success = True
    argsets = []
    for i,tbl in enumerate(selectionsql):
        argsets.append([filepath, tbl, selectionsql[tbl], i])

    # Now ingest each of the required tables into the DB.

    # Create multiple threads and dispatch them to run 
    # func execute_copy_via_shell() against each argset.
    # Then take all the boolean results and see if they all succeeded.
    # with PoolExecutor(max_workers=4) as executor:
    #     resultlist = executor.map(execute_copy_via_shell, argsets)
    #     success = all(resultlist)
    for argset in argsets:
        success = success and execute_copy_via_shell(argset)
    return success


def ingest_user_directory(argset): #rootdir, iidstr):
    """
    Given an interact_id number in string form and a path to a data dir,
    perform basic acceptance tests on the data files in that dir and
    if they pass, load them into the database, tagged with the iid.
    """
    [rootdir,iidstr] = argset
    # if not iidstr.endswith('90'):
    #     return
    dirpath = os.path.join(rootdir, iidstr)
    log("Attempting ingest on directory: %s" % dirpath)

    # get a list of datafiles in this directory
    candidate_files = []
    for fname in os.listdir(dirpath):
        filepath = os.path.join(dirpath, fname)
        if fname.endswith('.sdb') and iidstr in filepath:
            # then this is a sensedoc datafile
            candidate_files.append(filepath)
            mention("Adding target file: %s" % filepath)
        else:
            mention("Skipping target file: %s" % filepath)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_files = select_usable_files(candidate_files, iidstr)

    if not ingest_files:
        log("No usable files found for participant %s"%iidstr)
        return False

    mention("Directory contains ingestible files.")

    # now actually ingest the datafiles
    success = True
    argsets = []
    for sdbfile in ingest_files:
        # success = success and ingest_sdb_file(filepath, iidstr)
        argsets.append( [filepath, iidstr] )

    with PoolExecutor(max_workers=num_threads) as executor:
        resultlist = executor.map(ingest_sdb_file, argsets)
        success = all(resultlist)
    
    return success

if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    with open(logfile, 'w') as loghandle:
        log(args)

        # process ingest restrictions
        target_iids = []
        if args['-i']:
            target_iids.append(args['-i'])
            print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

        if args['PATH']:
            # Ingesting a single file is quick and easy.
            # (This is used primarily for testing.)
            if os.path.isfile(args['PATH']):
                fpath = args['PATH']
                if fpath.lower().endswith('.sdb'):
                    ingest_sdb_file( [fpath, "007"] )
                else:
                    err("Only .sdb files can be ingested.")
                exit()

        # Otherwise, we're ingesting directories recursively.
        rootdir = args['PATH']
        log("Root directory: %s" % rootdir)

        # The files we're looking for are organized in directories by
        # interact_id. The directory name is the participant's iid,
        # and the file within that directory is {iid}.sdb
        argsets = []
        for pid,dirpath in enumerate(os.listdir(rootdir)):
            if not target_iids or dirpath in target_iids:
                argsets.append( [rootdir,dirpath] )
                # argset = [rootdir,dirpath]
                # if ingest_user_directory( argset ):
                #     log("Participant %s ingested successfully."%dirpath)
                # else:
                #     log("Problem ingesting participant %s"%dirpath)

        with PoolExecutor(max_workers=num_threads) as executor:
            resultlist = executor.map(ingest_user_directory, argsets)
            # clean = all(resultlist)

