#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

NOTE:
This code has been instrumented to run multithreaded over the I/O
sections, but to make use of that, we need to run the job wrapped in
a bash shell that sets up the SLURM settings. Otherwise no additional
threads or processors will be available in the runtime environment.

Usage:
  sensedoc_ingest [options] PATH
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -L FNAME      Save log to FNAME
    -i IID        Ingest participant IID only
    -v,--verbose  Provide more verbose output
"""
import os
import sqlite3
import datetime
import subprocess as sub
from docopt import docopt
from itertools import combinations
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor as PoolExecutor

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 3600 # equates to 1 hour of usable data
dbname = 'interact_db'
logfile = datetime.datetime.now().strftime("ingest_%Y%m%d-%H%M%S.log")
loghandle = None

# a stat we can track to give confidence that the ingest worked
expected_gps_rows = 0
gps_file_row_counts = {}

def log(msg, prebreak=False):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s\n" % msg)
        loghandle.flush()
    if prebreak:
        print('')
    print("LOG: %s" % msg)

def mention(str):
    if args['--verbose']:
        print(str)
        # verbose statements written to the log as well as the screen
        # if loghandle:
            # loghandle.write("%s\n"%str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    """
    Given the start and end of two different date ranges, determine
    whether the ranges overlap.
    """
    # Make sure each start and end pair are in sorted order
    start1 = min(dt1_st, dt1_end)
    end1 = max(dt1_st, dt1_end)
    start2 = min(dt2_st, dt2_end)
    end2 = max(dt2_st, dt2_end)

    # If one range occurs entirely before the other, there is no overlap.
    # Otherwise there is.
    return not (end1 < start2 or start1 > end2)

def select_usable_files(filelist, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the set contains acceptable data, return a list of them. 
    Otherwise, return None.

    The entire set should be considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # pprint(filelist)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So to speed up the process, I'm assuming that if the gps table 
    # contains good dates and data, the accel table will as well. 
    # sql = """
    #         SELECT MIN(utcdate) as mindate,
    #                MAX(utcdate) as maxdate,
    #                COUNT(*) as count
    #         FROM gps
    #         """
            # It turns out that the following 3 queries are MUCH
            # faster than the one above, executing in about 1/8th 
            # the time. And since there's a non-trivial amount of 
            # time spent doing this, it's worth optimizing.
    sql_min = "SELECT utcdate from gps order by ts ASC limit 1"
    sql_max = "SELECT utcdate from gps order by ts DESC limit 1"
    sql_n   = "SELECT count(1) as n from gps"

    fstats = []
    for fpath in filelist:
        # open the file and compute its date range and record count
        try:
            # log("Connecting to source file %s"% fpath)
            with sqlite3.connect(fpath) as conn:
                c = conn.cursor()

                mindate = 0
                c.execute(sql_min)
                row = c.fetchone()
                if row:
                    mindate = row[0]

                maxdate = 0
                c.execute(sql_max)
                row = c.fetchone()
                if row:
                    maxdate = row[0]

                count = 0
                c.execute(sql_n)
                row = c.fetchone()
                if row:
                    count = int(row[0])

                # hang onto these counts for final stats
                # after the ingest is finished
                gps_file_row_counts[fpath] = count
                

        except Exception as e:
            log("Caught a SQL error while counting records.")
            log(e)

        # make a note of those stats if they're usable
        if mindate and maxdate and count:
            log("Keeping file %s"% fpath)
            fstats.append([mindate,maxdate,count,fpath])
            # good_files.append(fpath)
        else: # otherwise leave this file out of the list
            log("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                        mindate,
                                                        maxdate,
                                                        count))
    if len(fstats) > 1:
        # compare all possible pairings of files and reject 
        # this filelist if any pair has overlap between 
        # their min and max date stamps.
        log("Testing overlaps for %s"% iidstr)
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                log("Overlapping timestamps found between files:")
                log("  %s" % (stat1[3]))
                log("    %s -> %s"%(stat1[0], stat1[1])) 
                log("    %s -> %s"%(stat2[0], stat2[1])) 
                log("  %s" % (stat2[3]))
                return None
            else:
                log("Overlap clean.")
        # total the number of samples from all files and
        # reject the filelist if the number of samples does not
        # meet the minimum threshold
        contribution = sum([x[2] for x in fstats])
        if contribution < min_sample_threshold:
            log("Not enough samples from participant %s (%d)"%(iidstr,contribution))
            return None
        return fstats
    elif len(fstats) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return fstats

    log("No usable data files from participant %s." % iidstr)
    return None

def execute_copy_via_shell(filepath, tablename, selectionsql, pid):
    # On the command line we could run a single command to
    # pull data from the sqlite file with sqlite3 and ingest into the 
    # postgres by piping the output directly to psql.
    #
    # In simple form, that console command would be:
    # sqlite3 somefile.sdb SELECTIONSQL | psql interact_db INSERTSQL
    #
    # The prototype did all of this work in a series of Python Pandas 
    # scripts which made multiple passes of the datafiles to merge them
    # clean them, and then push them into the DB. But the resulting 
    # process was horribly slow. 
    #
    # This time, we're going to do it in a single pass of the data, and
    # we'll employ the sqlite3 and psql clients to do the heavy lifting. 
    # These tools have both been optimized for CSV streaming, and can 
    # load the data into the DB much more efficiently.
    #
    # In the simple form of the command line shown above, the two
    # key elements are SELECTIONSQL, which is the SQL fragment used to
    # select a table's content out of the target SQLite file, and the
    # INSERTSQL, which is the SQL fragment used to load the incoming
    # data into the PostgreSQL database.
    # 
    # In addition to doing the simple select/insert, these commands 
    # will also handle many of the cleaning and normalization steps
    # that need to be applied to the data before it can be used.
    #
    # Unfortunately, the data is known to contain occasional duplicate 
    # records, which happen when the SD device syncs its onboard clock 
    # with the real world. In these cases, the most accurate data for 
    # that timestamp are the values associated with the last record
    # recorded, not the first.
    #
    # So, to eliminate duplicates, we'll create a dummy table and load
    # the data into that, and then use INSERT ON CONFLICT DO NOTHING to
    # ignore duplicates as we copy the dummy table to the live table.
    
    # Create a suffix that will be appended to the temporary loading
    # tablename. This way, if we have multiple threads loading files,
    # they'll be stuffing distinct ingest tables and we can roll one
    # back cleanly on failure without compromising the others.
    tableid = '_%s_%s' % (filepath[-20:],tablename)

    # eliminate illegal chars in tableid
    tableid = tableid.translate({ord(i):None for i in '-./'})
    copycmd = """
             CREATE TABLE level_0.deleteme{tid} (LIKE level_0.{tn}); 
             COPY level_0.deleteme{tid} FROM STDIN delimiter '|' CSV;
             INSERT INTO level_0.{tn}
                SELECT * FROM level_0.deleteme{tid}
             ON CONFLICT (iid,ts) DO NOTHING;
             DROP TABLE level_0.deleteme{tid};
             """.format(tn=tablename,tid=tableid)

    # NOTE: There's still a minor issue here. We tend to want to ingest
    # an entire wave at a time, but if something goes wrong partway 
    # through, we don't have an elegant way to rollback the ingest and 
    # start it again. As it stands, running an ingest a second time 
    # should work fine, since the INSERT ON CONFLICT will simply ignore
    # records that were already ingested. This will admittedly take up 
    # a lot of time, loading all the previously ingested participants 
    # only to ignore their records at copy time. But processing the 
    # entire wave and ignoring records we already have seems the more
    # robust solution as it is less likely to accidentally omit data
    # by trying to be clever about skipping redundant ingests.

    cmdline = 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath,
                                                        selectionsql,
                                                        dbname,
                                                        copycmd)
    mention("Command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If any table fails, the ingest for that participant should be 
        # reported as unsuccessful.
        log("Ingesting file %s failed with return code: %s"%(filepath,res))
        return False
    return True

def ingest_sdb_file(filepath,iidstr):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    refDate = ''
    # We counted file rows when we were validating the files, 
    # so now that we know this file is being ingested, add its
    # row count to the total number of rows we're trying to ingest.
    global expected_gps_rows
    if filepath in gps_file_row_counts:
        expected_gps_rows += gps_file_row_counts[filepath]
    else:
        log("SDB FILE '%s' was never row-counted.")

    log("Ingesting file: %s"%filepath)
    with sqlite3.connect(filepath) as conn:
        # get the reference date from the SDB from which all 
        # timestamps are measured
        c = conn.cursor()
        c.execute("SELECT value FROM ancillary WHERE key='refDate'")
        refDate = c.fetchone()[0]


    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

    # Each table in the SQLite file needs its own SELECT stmt
    # to ensure data is read from the file in a known format.
    selectionsql = {}
    selectionsql['sd_accel'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---convert to 0.0-1.0
                            y * 0.00390625 as y, ---convert to 0.0-1.0
                            z * 0.00390625 as z  ---convert to 0.0-1.0
                       FROM accel
                       WHERE x is not null
                         AND y is not null
                         AND z is not null
                       ORDER BY ts, ROWID DESC;
                       """.format(iidstr, refDate)
    selectionsql['sd_gps'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            lat, lon, speed, course, 
                            -- mode, fix, 
                            alt, 
                            -- mode1, mode2, 
                            sat_used, 
                            pdop, hdop, vdop, sat_in_view 
                            FROM gps
                            WHERE lat is not null
                              AND lon is not null
                              AND lat BETWEEN -90 and 90
                              AND lon BETWEEN -180 and 180
                              AND alt BETWEEN -10000 and 100000
                              AND course BETWEEN -360 and 360
                              AND speed BETWEEN 0 and 1000
                              AND sat_used BETWEEN 0 and 50 
                              AND sat_in_view BETWEEN 0 and 50
                            ORDER BY ts, ROWID DESC;
                        """.format(iidstr, refDate)
        # NOTE: In the above queries, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of 1000000.0, 
        # to preserve the floating-point nature of the result.
        # The rows are sorted by ts, and then  ROWID DESC because 
        # we need to filter later by the order in which the rows were 
        # written to the file, keeping only the latest instance in cases
        # where records share the same timestamp info.

    # Now ingest each of the required tables into the DB.
    success = True
    for i,tbl in enumerate(selectionsql):
        success = success and execute_copy_via_shell(filepath, tbl, 
                                                     selectionsql[tbl], 
                                                     i)
    return success


def ingest_user_files(iidstr, filepathlist):
    """
    Given an interact_id string and a list of filepaths, 
    perform basic acceptance tests on the data files and
    if they pass, load them into the database, tagged with the iid.
    """
    log("Ingesting files for user: %s" % iidstr, prebreak=True)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_file_stats = select_usable_files(filepathlist, iidstr)

    if not ingest_file_stats:
        log("Files found for participant %s were not usable."%iidstr)
        return False

    log("User %s has ingestible files."%iidstr)

    # now actually ingest the datafiles
    success = True
    for [min_dt, max_dt, count, sdbfile] in ingest_file_stats:
        result = ingest_sdb_file(sdbfile,iidstr)
        success = success and result

    return success

import psycopg2
# from sqlalchemy import create_engine
db_host = os.environ["SQL_LOCAL_SERVER"]
db_host_port = int(os.environ["SQL_LOCAL_PORT"])
db_user = os.environ["SQL_USER"]
db_name = os.environ["SQL_DB"]
db_schema = os.environ["SQL_SCHEMA"]

def count_db_rows(tablename):
    """
    Given the name of a table in the Postgres DB, return 
    the row-count from that table.
    """
    # dbURI = 'postgresql://{}@{}:{}/{}'.format(db_user,
    #                                 db_host, db_host_port,
    #                                 db_name)
    # engine = create_engine(dbURI)
    # engine.execute(q)
    count=-1
    with psycopg2.connect(user=db_user,
                          host=db_host,
                          port=db_host_port,
                          database=db_name) as conn:
        c = conn.cursor()
        sql = "SELECT count(1) FROM %s.%s" % (db_schema,tablename)
        c.execute(sql)
        count = int(c.fetchone()[0])
    return count



if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    # if user has specified a log filename, use that instead 
    # of the default
    if args['-L']:
        logfile = args['-L']

    with open(logfile, 'w') as loghandle:
        log("Ingest script was run with following arguments:")
        log(args)

        # process ingest restrictions
        target_iids = []
        if args['-i']:
            target_iids.append(args['-i'])
            print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

        if args['PATH']:
            # Ingesting a single file is quick and easy.
            # (This is used primarily for testing.)
            if os.path.isfile(args['PATH']):
                fpath = args['PATH']
                if fpath.lower().endswith('.sdb'):
                    ingest_sdb_file( [fpath, "007"] )
                else:
                    err("Only .sdb files can be ingested.")
                exit()

        # Otherwise, we ingest directories recursively.
        rootdir = args['PATH']
        log("Root directory: %s" % rootdir)

        # The files we're looking for are organized in directories by
        # the unique pairing of interact_id and sdid. 
        # The directory name is the participant's iid, followed by the 
        # id of the SD device, so the sdb files for a 
        # particular participant can appear in multiple folders. 
        # argsets = []
        # for pid,dirpath in enumerate(os.listdir(rootdir)):
        #     if not target_iids or dirpath in target_iids:
        #         argsets.append( [rootdir,dirpath] )

        # So there's no point in trying to ingest by directory, because
        # a single participant's SDBs can be found in multiple
        # directories. What makes more sense is to ingest all the 
        # specific sdb files associated with a particular user.
        participant_files = {}
        for foldername in os.listdir(rootdir):
            folderpath = os.path.join(rootdir, foldername)
            if os.path.isdir(folderpath): # we have a data folder
                sdidstr = ''
                iidstr = foldername
                # sdb data for some participants split over mult folders
                if '_' in foldername:
                    iidstr,dummy,sdidstr = foldername.partition('_')
                # all sdb data within folder belongs to given participant
                for filename in os.listdir(folderpath):
                    filepath = os.path.join(folderpath, filename)
                    if filename.lower().endswith('.sdb'):
                        if not iidstr in participant_files:
                            participant_files[iidstr] = []
                        participant_files[iidstr].append(filepath)

        # Before we ingest the files, take a row count from the existing
        # DB tables so that we can count again post-ingest and compare
        # against the number of rows we intended to ingest from each
        # of the SDB files. This will give some measure of confidence
        # as to whether the ingest was successful.
        # In this pre-ingest count, we report both GPS and Accel counts,
        # despite the higher cost of counting accel rows, so that we
        # have a rollback point if the ingest crashes.
        baseline_gps_rows = count_db_rows('sd_gps')
        baseline_accel_rows = count_db_rows('sd_accel')
        log("Baseline GPS Count: %d"%baseline_gps_rows)
        log("Baseline Accel Count: %d"%baseline_accel_rows)

        # We now have a dictionary that maps each participant id to
        # a list of their sdb filepaths.
        for iidstr in participant_files:
            if participant_files[iidstr]:
                result = ingest_user_files(iidstr,
                                           participant_files[iidstr])

        # Count final GPS table size
        final_gps_rows = count_db_rows('sd_gps') 

        # Report before, after, and expected row counts. After the
        # initial baseline, we only track GPS table rows, as they are 
        # much smaller than the accel tables, so they're much faster to 
        # compute and still provide a reasonable metric of success.
        log("Baseline GPS Count: %d"%baseline_gps_rows)
        log("Expected GPS Count: %d"%(baseline_gps_rows+expected_gps_rows))
        log("Complete GPS Count: %d"%final_gps_rows)
        ingested_gps_rows = final_gps_rows - baseline_gps_rows
        ingest_rate = 100.0*ingested_gps_rows/float(expected_gps_rows)
        if ingest_rate > 99.5:
            log("Ingest looks successful with %0.2f%% row retention. Remember, some minor row loss is expected due to timestamp duplication in SDB files."%ingest_rate)
        else:
            log("Row retention of %0.2f%% looks low."%ingest_rate) 
            log("Minor row loss is expected due to duplicated timestamps in the SDB files, but this seems excessive.")
            log("But if this was a re-ingest of an uncompleted previous ingest cycle, high row loss is to be expected.")

