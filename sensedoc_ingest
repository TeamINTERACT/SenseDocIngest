#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

NOTE:
This code has been instrumented to run multithreaded over the I/O
sections, but to make use of that, we need to run the job wrapped in
a bash shell that sets up the SLURM settings. Otherwise no additional
threads or processors will be available in the runtime environment.

Usage:
  sensedoc_ingest [options] PATH
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -i IID        Ingest participant IID only
    -v,--verbose  Provide more verbose output
"""
import os
import sqlite3
import datetime
import subprocess as sub
from docopt import docopt
from itertools import combinations
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor as PoolExecutor

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 3600 # equates to 1 hour of usable data
dbname = 'interact_db'
logfile = datetime.datetime.now().strftime(".ingest_%Y%m%d-%H%M%S.log")
loghandle = None

# number of processing threads to run
num_threads = 1

def log(msg, prebreak=False):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s\n" % msg)
        loghandle.flush()
    if prebreak:
        print('')
    print("LOG: %s" % msg)

def mention(str):
    if args['--verbose']:
        print(str)
        # verbose statements written to the log as well as the screen
        # if loghandle:
            # loghandle.write("%s\n"%str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    """
    Given the start and end of two different date ranges, determine
    whether the ranges overlap.
    """
    # Make sure each start and end pair are in sorted order
    start1 = min(dt1_st, dt1_end)
    end1 = max(dt1_st, dt1_end)
    start2 = min(dt2_st, dt2_end)
    end2 = max(dt2_st, dt2_end)

    # If one range occurs entirely before the other, there is no overlap.
    # Otherwise there is.
    return not (end1 < start2 or start1 > end2)

def select_usable_files(filelist, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the set contains acceptable data, return a list of them. 
    Otherwise, return None.

    The entire set should be considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # pprint(filelist)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So to speed up the process, I'm assuming that if the gps table 
    # contains good dates and data, the accel table will as well. 
    # sql = """
    #         SELECT MIN(utcdate) as mindate,
    #                MAX(utcdate) as maxdate,
    #                COUNT(*) as count
    #         FROM gps
    #         """
            # It turns out that the following 3 queries are MUCH
            # faster than the one above, executing in about 1/8th 
            # the time. And since there's a non-trivial amount of 
            # time spent doing this, it's worth optimizing.
    sql_min = "SELECT utcdate from gps order by ts ASC limit 1"
    sql_max = "SELECT utcdate from gps order by ts DESC limit 1"
    sql_n   = "SELECT count(1) as n from gps"

    fstats = []
    for fpath in filelist:
        # open the file and compute its date range and record count
        try:
            # log("Connecting to source file %s"% fpath)
            with sqlite3.connect(fpath) as conn:
                c = conn.cursor()

                mindate = 0
                c.execute(sql_min)
                row = c.fetchone()
                if row:
                    mindate = row[0]

                maxdate = 0
                c.execute(sql_max)
                row = c.fetchone()
                if row:
                    maxdate = row[0]

                count = 0
                c.execute(sql_n)
                row = c.fetchone()
                if row:
                    count = row[0]

        except Exception as e:
            log("Caught a SQL error while counting records.")
            log(e)

        # make a note of those stats if they're usable
        if mindate and maxdate and count:
            log("Keeping file %s"% fpath)
            fstats.append([mindate,maxdate,count,fpath])
            # good_files.append(fpath)
        else: # otherwise leave this file out of the list
            log("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                        mindate,
                                                        maxdate,
                                                        count))
    if len(fstats) > 1:
        # compare all possible pairings of files and reject 
        # this filelist if any pair has overlap between 
        # their min and max date stamps.
        log("Testing overlaps for %s"% iidstr)
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                spanstr = "   %s - %s\n   %s - %s"%(stat1[0], stat1[1], 
                                                stat2[0], stat2[1])
                log("Overlapping timestamps found between files:\n  %s\n  %s" % (stat1[3],stat2[3]))
                return None
            else:
                log("Overlap clean.")
        # total the number of samples from all files and
        # reject the filelist if the number of samples does not
        # meet the minimum threshold
        contribution = sum([x[2] for x in fstats])
        if contribution < min_sample_threshold:
            log("Not enough samples from participant %s (%d)"%(iidstr,contribution))
            return None
        return fstats
    elif len(fstats) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return fstats

    log("No usable data files from participant %s." % iidstr)
    return None

def execute_copy_via_shell(argset):
    # The easiest way to invoke this func from the thread pool manager
    # is to pack up the arguments for each call into a list. So first
    # we have to unpack the function arguments.
    [filepath, tablename, selectionsql, pid] = argset

    # Now we have everything we need to ingest the given datafile. 
    # On the command line we could run a single command to
    # pull data from the sqlite file with sqlite3 and ingest into the 
    # postgres by piping the output directly to psql.
    #
    # In simple form, that console command would be:
    # sqlite3 somefile.sdb SELECTIONSQL | psql interact_db INSERTSQL
    #
    # The prototype did all of this work in a series of Python Pandas 
    # scripts which made multiple passes of the datafiles to merge them
    # clean them, and then push them into the DB. But the resulting 
    # process was horribly slow. 
    #
    # This time, we're going to do it in a single pass of the data, and
    # we'll employ the sqlite3 and psql clients to do the heavy lifting. 
    # These tools have both been optimized for CSV streaming, and can 
    # load the data into the DB much more efficiently.
    #
    # In the simple form of the command line shown above, the two
    # key elements are SELECTIONSQL, which is the SQL fragment used to
    # select a table's content out of the target SQLite file, and the
    # INSERTSQL, which is the SQL fragment used to load the incoming
    # data into the PostgreSQL database.
    # 
    # In addition to doing the simple select/insert, these commands 
    # will also handle many of the cleaning and normalization steps
    # that need to be applied to the data before it can be used.
    #
    # Unfortunately, the data is known to contain occasional duplicate 
    # records, which happen when the SD device syncs its onboard clock 
    # with the real world. In these cases, the most accurate data for 
    # that timestamp are the values associated with the last record
    # recorded, not the first.
    #
    # So, to eliminate duplicates, we'll create a dummy table and load
    # the data into that, and then use INSERT ON CONFLICT DO NOTHING to
    # ignore duplicates as we copy the dummy table to the live table.
    # print("Filepath: %s"%filepath)
    # print("Tablename: %s"%tablename)
    # print("PID: %s"%pid)
    # tableid = '_%s_%s' % (tablename, pid)
    tableid = '_%s_%s' % (filepath[-20:],tablename)
    tableid = tableid.translate({ord(i):None for i in '-./'})
    # tableid = ''
    copycmd = """
             CREATE TABLE level_0.deleteme{tid} (LIKE level_0.{tn}); 
             COPY level_0.deleteme{tid} FROM STDIN delimiter '|' CSV;
             INSERT INTO level_0.{tn}
                SELECT * FROM level_0.deleteme{tid}
             ON CONFLICT (iid,ts) DO NOTHING;
             DROP TABLE level_0.deleteme{tid};
             """.format(tn=tablename,tid=tableid)

    cmdline = 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath,
                                                        selectionsql,
                                                        dbname,
                                                        copycmd)
    mention("Command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If any table fails, the ingest for that participant should be 
        # reported as unsuccessful.
        log("Ingesting file %s failed with return code: %s"%(filepath,res))
        return False
    return True

def ingest_sdb_file(argset):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    [filepath,iidstr] = argset
    refDate = ''
    log("Ingesting file: %s"%filepath)
    with sqlite3.connect(filepath) as conn:
        # get the reference date from the SDB from which all 
        # timestamps are measured
        c = conn.cursor()
        c.execute("SELECT value FROM ancillary WHERE key='refDate'")
        refDate = c.fetchone()[0]

    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

    # From the prototype system, the fuse_sd_pdata.py script contains
    # a number of data scrubbing and polishing steps. These same steps
    # need to be implemented in this newer (and hopefully faster) ingest
    # system.
    # X Reject filesets with overlapping time windows (and notify admin)
    # X Add the participant's iid to every sample record
    # X Merge the timestamps into one value with millisec precision
    # X Scale the x,y,z values of the accel table
    # X Set empty string as default value for all text and char fields
    # X All fields must be NOT NULL and have sensible defaults
    # X gps.lat must be between -90 and 90 inclusive
    # X gps.lon must be between -180 and 180 inclusive
    # X gps.sat_in_view to integer and default to -1
    # - Reject records that duplicate existing value of iid,ts
    #      Unfortunately, the records with duplicate timestamps are 
    #      not duplicated in all fields, so we can't just do
    #      a simple SAMPLE DISTINCT when exporting from the SQLite file.
    #      Consequently, the most efficient solution I've found is to
    #      load the file into a dummy DB table and then copy from there 
    #      into the actual table with INSERT ON CONFLICT DO NOTHING.
    #      It requires a second pass of the ingested data, but it's a
    #      pretty efficient second pass.

    # Each table in the SQLite file needs its own SELECT stmt
    # to ensure data is read from the file in a known format.
    selectionsql = {}
    selectionsql['sd_accel'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---convert to 0.0-1.0
                            y * 0.00390625 as y, ---convert to 0.0-1.0
                            z * 0.00390625 as z  ---convert to 0.0-1.0
                       FROM accel
                       WHERE x is not null
                         AND y is not null
                         AND z is not null
                       ORDER BY ts, ROWID DESC;
                       """.format(iidstr, refDate)
    selectionsql['sd_gps'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            lat, lon, speed, course, 
                            -- mode, fix, 
                            alt, 
                            -- mode1, mode2, 
                            sat_used, 
                            pdop, hdop, vdop, sat_in_view 
                            FROM gps
                            WHERE lat is not null
                              AND lon is not null
                              AND lat BETWEEN -90 and 90
                              AND lon BETWEEN -180 and 180
                              AND alt BETWEEN -10000 and 100000
                              AND course BETWEEN -360 and 360
                              AND speed BETWEEN 0 and 1000
                              AND sat_used BETWEEN 0 and 50 
                              AND sat_in_view BETWEEN 0 and 50
                            ORDER BY ts, ROWID DESC;
                        """.format(iidstr, refDate)
        # NOTE: In the above queries, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of 1000000.0, 
        # to preserve the floating-point nature of the result.
        # The rows are sorted by ts, and then  ROWID DESC because 
        # we need to filter later by the order in which the rows were 
        # written to the file, keeping only the latest instance in cases
        # where records share the same timestamp info.

    # Also need a field assignement stmt for the ON CONFLICT clause
    # updatesql = {}
    # updatesql['sd_gps'] = """
    #                 SET lat = EXCLUDED.lat,
    #                     lon = EXCLUDED.lon,
    #                     speed = EXCLUDED.speed,
    #                     course = EXCLUDED.course,
    #                     alt = EXCLUDED.alt,
    #                     sat_used = EXCLUDED.sat_used,
    #                     pdop = EXCLUDED.pdop,
    #                     hdop = EXCLUDED.hdop,
    #                     vdop = EXCLUDED.vdop,
    #                     sat_in_view = EXCLUDED.sat_in_view
    #                     """
    # updatesql['sd_accel'] = """
    #                 SET x = EXCLUDED.x,
    #                     y = EXCLUDED.y,
    #                     z = EXCLUDED.z
    #                     """

    # In a moment, we're going to ingest the individual files into the 
    # DB, but to do it quickly, we're going to dispatch multiple 
    # threads. In preparation for that step, we're going to
    # pre-construct the set of parameters that will be passed to the
    # actual ingestor function - one set of args for each invocation.
    success = True
    argsets = []
    for i,tbl in enumerate(selectionsql):
        argsets.append([filepath, tbl, 
                        selectionsql[tbl], 
                        # updatesql[tbl], 
                        i])

    # Now ingest each of the required tables into the DB.

    # Create multiple threads and dispatch them to run 
    # func execute_copy_via_shell() against each argset.
    # Then take all the boolean results and see if they all succeeded.
    # with PoolExecutor(max_workers=4) as executor:
    #     resultlist = executor.map(execute_copy_via_shell, argsets)
    #     success = all(resultlist)
    for argset in argsets:
        success = success and execute_copy_via_shell(argset)
# I NEED TO COMPARE NUMBER OF RECORDS INGESTED AGAINST NUMBER
# ORIGINALLY COUNTED IN THE FILE
    return success


def ingest_user_files(argset):
    """
    Given an interact_id string and a list of filepaths, 
    perform basic acceptance tests on the data files and
    if they pass, load them into the database, tagged with the iid.
    Argset is expected to contain: 
              iidstr: the interact_id of the participant
        filepathlist: the list of files to be ingested
    """
    [iidstr, filepathlist] = argset
    log("Ingesting files for user: %s" % iidstr, prebreak=True)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_file_stats = select_usable_files(filepathlist, iidstr)

    if not ingest_file_stats:
        log("Files found for participant %s were not usable."%iidstr)
        return False

    log("User %s has ingestible files."%iidstr)

    # now actually ingest the datafiles
    success = True
    argsets = []
    for [min_dt, max_dt, count, sdbfile] in ingest_file_stats:
        argsets.append( [sdbfile, iidstr] )

# I NEED TO TEST THE NUMBER OF RECORDS INGESTED VS THE NUMBER 
# ORIGINALLY COUNTED IN THE FILES AND REPORT THE NUMBER OF CULLED
# RECORDS IN THE LOG
    with PoolExecutor(max_workers=num_threads) as executor:
        resultlist = executor.map(ingest_sdb_file, argsets)
        success = all(resultlist)
    
    return success

if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    with open(logfile, 'w') as loghandle:
        log("Ingest script was run with following arguments:")
        log(args)

        # process ingest restrictions
        target_iids = []
        if args['-i']:
            target_iids.append(args['-i'])
            print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

        if args['PATH']:
            # Ingesting a single file is quick and easy.
            # (This is used primarily for testing.)
            if os.path.isfile(args['PATH']):
                fpath = args['PATH']
                if fpath.lower().endswith('.sdb'):
                    ingest_sdb_file( [fpath, "007"] )
                else:
                    err("Only .sdb files can be ingested.")
                exit()

        # Otherwise, we ingest directories recursively.
        rootdir = args['PATH']
        log("Root directory: %s" % rootdir)

        # The files we're looking for are organized in directories by
        # the unique pairing of interact_id and sdid. 
        # The directory name is the participant's iid, followed by the 
        # id of the SD device, so the sdb files for a 
        # particular participant can appear in multiple folders. 
        argsets = []
        for pid,dirpath in enumerate(os.listdir(rootdir)):
            if not target_iids or dirpath in target_iids:
                argsets.append( [rootdir,dirpath] )

        # So there's no point in trying to ingest a directory. What makes
        # more sense is to ingest all the specific sdb files associated
        # with a particular user.
        participant_files = {}
        for foldername in os.listdir(rootdir):
            folderpath = os.path.join(rootdir, foldername)
            if os.path.isdir(folderpath): # we have a data folder
                sdidstr = ''
                iidstr = foldername
                # sdb data for some participants split over mult folders
                if '_' in foldername:
                    iidstr,dummy,sdidstr = foldername.partition('_')
                # all sdb data within folder belongs to given participant
                for filename in os.listdir(folderpath):
                    filepath = os.path.join(folderpath, filename)
                    if filename.lower().endswith('.sdb'):
                        if not iidstr in participant_files:
                            participant_files[iidstr] = []
                        participant_files[iidstr].append(filepath)

        # we now have a dictionary that maps each participant id to
        # a list of sdb filepaths, which may or may not occur in 
        # different folders of the archive tree
        argsets = []
        for iidstr in participant_files:
            if participant_files[iidstr]:
                argsets.append( [iidstr, participant_files[iidstr]] )

        with PoolExecutor(max_workers=num_threads) as executor:
            resultlist = executor.map(ingest_user_files, argsets)
            # clean = all(resultlist)

