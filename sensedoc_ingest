#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

NOTE:
This code has been instrumented to run multithreaded over the I/O
sections, but to make use of that, we need to run the job wrapped in
a bash shell that sets up the SLURM settings. Otherwise no additional
threads or processors will be available in the runtime environment.

NOTE:
After discussion with ComputeCanada IT, they discourage running this kind of job in parallel because opening and closing lots of smaller files actually degrades performance, given the underlying structures used for the file system.

Usage:
  sensedoc_ingest [options] PATH CITYID WAVEID STREAMID
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -H DIR        Run data hygiene scripts from DIR on incoming data
    -L FNAME      Save log to FNAME
    -D            Detect row leakage
    -i IID        Ingest participant IID only
    -E            Display runtime environment vars and then quit 
    -v,--verbose  Provide more verbose output
"""
import os
import re
import sqlite3
import datetime
import psycopg2
import subprocess as sub
from tqdm import tqdm
from docopt import docopt
from itertools import combinations
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor as PoolExecutor

try:
    #db_name = 'interact_db'
    db_host = os.environ["SQL_LOCAL_SERVER"]
    db_host_port = int(os.environ["SQL_LOCAL_PORT"])
    db_user = os.environ["SQL_USER"]
    db_name = os.environ["SQL_DB"]
    db_schema = os.environ["SQL_SCHEMA"]
except KeyError as err:
    print("A required runtime environment variable was not found.")
    print(err)
    print("Have you set up the run-time secrets?")
    exit(1)

hygiene_dir = ''

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 3600 # equates to 1 hour of usable data
detect = True
purge = False
logfile = datetime.datetime.now().strftime("tmp_ingest_%Y%m%d-%H%M%S.log")
loghandle = None

# a stat we can track to give confidence that the ingest worked
expected_gps_rows = 0
gps_file_row_counts = {}

def log(msg, prebreak=False, screen=False):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s\n" % msg)
        loghandle.flush()
    if screen:
        if prebreak:
            print('')
        print("LOG: %s" % msg)

def mention(str):
    if args['--verbose']:
        print(str)
        # verbose statements written to the log as well as the screen
        # if loghandle:
            # loghandle.write("%s\n"%str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    """
    Given the start and end of two different date ranges, determine
    whether the ranges overlap.
    """
    # Make sure each start and end pair are in sorted order
    start1 = min(dt1_st, dt1_end)
    end1 = max(dt1_st, dt1_end)
    start2 = min(dt2_st, dt2_end)
    end2 = max(dt2_st, dt2_end)

    # If one range occurs entirely before the other, there is no overlap.
    # Otherwise there is.
    return not (end1 < start2 or start1 > end2)

def select_usable_files(filelist, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the set contains acceptable data, return a list of them. 
    Otherwise, return None.

    The entire set should be considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # pprint(filelist)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So to speed up the process, I'm assuming that if the gps table 
    # contains good dates and data, the accel table will as well. 
    sql_min = "SELECT utcdate from gps order by ts ASC limit 1"
    sql_max = "SELECT utcdate from gps order by ts DESC limit 1"
    sql_n   = "SELECT count(1) as n from gps"

    fstats = []
    for fpath in filelist:
        # open the file and compute its date range and record count
        try:
            # log("Connecting to source file %s"% fpath)
            with sqlite3.connect(fpath) as conn:
                c = conn.cursor()

                mindate = 0
                c.execute(sql_min)
                row = c.fetchone()
                if row:
                    mindate = row[0]

                maxdate = 0
                c.execute(sql_max)
                row = c.fetchone()
                if row:
                    maxdate = row[0]

                count = 0
                c.execute(sql_n)
                row = c.fetchone()
                if row:
                    count = int(row[0])

                # hang onto these counts for final stats
                # after the ingest is finished
                gps_file_row_counts[fpath] = count
                

        except Exception as e:
            log("Caught a SQL error while counting records.")
            log(e)

        # make a note of those stats if they're usable
        if mindate and maxdate and count:
            log("Keeping file %s"% fpath)
            fstats.append([mindate,maxdate,count,fpath])
        else: # otherwise leave this file out of the list
            log("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                        mindate,
                                                        maxdate,
                                                        count))
    if len(fstats) > 1:
        # compare all possible pairings of files and reject 
        # this filelist if any pair has overlap between 
        # their min and max date stamps.
        log("Testing overlaps for %s"% iidstr)
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                log("Overlapping timestamps found between files:")
                log("  %s" % (stat1[3]))
                log("    %s -> %s"%(stat1[0], stat1[1])) 
                log("    %s -> %s"%(stat2[0], stat2[1])) 
                log("  %s" % (stat2[3]))
                return None
            else:
                log("Overlap clean.")
        # total the number of samples from all files and
        # reject the filelist if the number of samples does not
        # meet the minimum threshold

        # COULD THIS BE WHERE THE HIGH NUM OF DROPPED ROWS COMING FROM?
        # No, be, because files that get dropped here never get sent
        # to the ingestor, so their counts not added to expected_gps_rows

        contribution = sum([x[2] for x in fstats])
        if contribution < min_sample_threshold:
            log("Not enough samples from participant %s (%d)"%(iidstr,contribution))
            return None
        return fstats
    elif len(fstats) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return fstats

    log("No usable data files from participant %s." % iidstr)
    return None

def indent_block(instr):
    return '\n'.join(['  '+x for x in instr.strip().split('\n')])

def run_data_hygiene_scripts(city,wave,stream,iid,tablename,serial,scriptdir):
    """
    The raw data being ingested is often messy. It contains
    occasional duplicate records, records with illegal values,
    corrupted entries, mislabeled entries, etc.

    These data problems come in two broad categories: some are
    common across all the incoming files, while others are
    specific to particular files, or only occur occasionally
    under very specific conditions. Because of this, we need a
    generic, flexible way to extend the ingest process so that it
    can respond to arbitrarily complex situations.

    To that end, I'm implementing a system of external scripts.
    Any scripts found in the scriptdir directory will be run
    against the incoming data table so that it can work its
    magic. The scripts are expected to be named in the form
    NNN-some-description.* so that we can control the order of
    execution by changing the 3-digit NNN prefix.

    For maximum flexibility, each script will be required to
    handle its own connection to the DB. Any output generated by
    the script will be entered into the runtime log of this
    parent ingest script. The script is expected to return 0 if
    it runs successfully, 1 if it fails, and 2 if it succeeds but
    has determined that the table cannot be ingested. (Which
    means we can stop running additional scripts against it.)
    """

    # for each script in the script directory
    success_code_all = 0
    if scriptdir:
        for root,dirs,files in os.walk(scriptdir):
            # prep the parameters to the script
            # run the script
            for script in sorted(files):
                # if it's a prioritized script file
                if re.match('\d\d-', script):
                    scriptpath = os.path.abspath(os.path.join(root,script))
                    cmd = [scriptpath, iid, 
                           str(city), str(wave), 
                           tablename, serial, '-e']
                    # pprint(cmd)
                    cp = sub.run(cmd, stdout=sub.PIPE)
                    success_code_all = max([success_code_all,
                                            cp.returncode])
                    # log script output and make it easy to 
                    # parse visually in log file
                    log("BEGIN SCRIPT LOG: %s on %s"%(script,tablename))
                    log(indent_block(cp.stdout.decode('utf-8')))
                    log("END SCRIPT LOG: %s"%script)
                else:
                    mention("%s is not a script file."%script)
    return(success_code_all) # return successful completion

def execute_copy_via_shell(filepath, city,wave,stream,iid, 
                           tablename, selectionsql):
    """
    On the command line we could run a single command to
    pull data from the sqlite file with sqlite3 and ingest into the 
    postgres by piping the output directly to psql.

    In simple form, that console command would be:
    sqlite3 somefile.sdb SELECTIONSQL | psql interact_db INSERTSQL

    In the simple form of the command line shown above, the two
    key elements are SELECTIONSQL, which is the SQL fragment used to
    select a table's content out of the target SQLite file, and the
    INSERTSQL, which is the SQL fragment used to load the incoming
    data into the PostgreSQL database.
    
    Unfortunately, the data is known to contain occasional duplicate 
    records, which happen when the SD device syncs its onboard clock 
    with the real world. In these cases, the most accurate data for 
    that timestamp are the values associated with the last record
    recorded, not the first.

    So during the export from the sqlite sdb file, we add a uniqueness
    clause to be sure that we only take the latest copy of any records with duplicated timestamps.
    """
    
    # Create an identifier to append to the temporary loading
    # tablename. This way, if a table doesn't pass verification, we
    # can leave it in place for forensic investigation and subsequent
    # file ingests will not try to overwrite it.
    tail = ''
    serial = ''
    basename = os.path.basename(filepath)
    serialpat = "SD(?P<num>\d*)fw(?P<rev>\d*)_(?P<tail>.*)\.sdb"
    m = re.match(serialpat, basename)
    if m:
        if not m.group('num') or not m.group('rev'):
            log("Problem getting serial from %s" % basename)
        serial = "%s-%s" % (m.group('num'), m.group('rev'))
        tail = m.group('tail')
    # some users have multiple sdb files that only differ
    # toward the end of their filename. Add that as well
    # to ensure unique temp table names
    if not tail:
        log("SDB tail empty. Wassup with that?")
    tableid = '_%s_%s_%s_%s' % (iid,serial,tail,tablename)

    # eliminate illegal chars in tableid
    tableid = tableid.translate({ord(i):None for i in '-./'})
    full_tablename = "%s.%s" % (db_schema, tablename)
    raw_tablename = "deleteme%s" % (tableid)
    nodups_tablename = "deletemetoo%s" % (tableid)
    idx_name = "deletemetoo%s_btree_idx" % (tableid)
    ingestcmd = """
             SET SCHEMA '{sch}';
             CREATE TABLE {rawtbl} (LIKE {maintbl}); 
             -- load the raw table from the sqlite stream
             COPY {rawtbl} FROM STDIN delimiter '|' CSV;
             """.format(maintbl=tablename, rawtbl=raw_tablename, 
                        # nodupstbl=nodups_tablename,
                        idx=idx_name, sch=db_schema)
    absorbcmd = """
             SET SCHEMA '{sch}';
             INSERT INTO {maintbl}
                SELECT * FROM {rawtbl};
             DROP TABLE {rawtbl};
             """.format(maintbl=tablename, rawtbl=raw_tablename, 
                        # nodupstbl=nodups_tablename,
                        idx=idx_name, sch=db_schema)
             # for debugging purposes, the rawtable can be kept in DB
             # by prefixing -- to the DROP TABLE line
             # But be cautious: it doubles the disk footprint of the
             # ingested data.

    # NOTE: There's still a minor issue here. We tend to want to ingest
    # an entire wave at a time, but if something goes wrong partway 
    # through, we don't have an elegant way to rollback the ingest and 
    # start it again. As it stands, running an ingest a second time 
    # should work fine, since the INSERT ON CONFLICT will simply ignore
    # records that were already ingested. This will admittedly take up 
    # more time, loading all the previously ingested participants 
    # only to ignore their records at copy time. But processing the 
    # entire wave and ignoring records we already have seems the more
    # robust solution as it is less likely to accidentally omit data
    # by trying to be clever about skipping redundant ingests.

    # Load the raw data into a temp table
    cmdline = 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath, 
                                                        selectionsql,
                                                        db_name, 
                                                        ingestcmd)
    mention("Load command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If table load fails, the ingest for that participant should be 
        # reported as unsuccessful.
        log("Ingesting file %s failed with return code: %s"%(filepath,res))
        return False

    # Run the hygiene scripts on the temp table
    res = run_data_hygiene_scripts(city,wave,stream,iid,
                                   raw_tablename, serial, hygiene_dir)
    if res:
        # If hygiene fails, the ingest for that participant should be 
        # reported as unsuccessful.
        log("Ingesting file %s failed hygiene tests with return code: %s"%(filepath,res))
        return False

    # If the hygiene scripts succeeded, transfer temp table into main
    cmdline = 'psql %s -q -c "%s"' % (db_name, absorbcmd)
    mention("Absorb command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If any table fails, the ingest for that participant should 
        # be reported as unsuccessful.
        log("Absorbing file %s failed with return code: %s"%(filepath,res))
        return False

    # OTHERWISE, SHOULDN'T WE DROP THE TEMP TABLE?

    return True

def ingest_sdb_file(filepath,city,wave,stream,iidstr):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    refDate = ''
    # We counted file rows when we were validating the files, 
    # so now that we know this file is being ingested, add its
    # row count to the total number of rows we're trying to ingest.
    global expected_gps_rows
    if filepath in gps_file_row_counts:
        expected_gps_rows += gps_file_row_counts[filepath]
    else:
        log("SDB FILE '%s' was never row-counted.")

    log("Ingesting file: %s"%filepath)
    with sqlite3.connect(filepath) as conn:
        # get the reference date from the SDB from which all 
        # timestamps are measured
        c = conn.cursor()
        c.execute("SELECT value FROM ancillary WHERE key='refDate'")
        refDate = c.fetchone()[0]


    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

    # Each table in the SQLite file needs its own SELECT stmt
    # to ensure data is read from the file in a known format.
    selectionsql = {}
    selectionsql['sd_accel'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---convert to 0.0-1.0
                            y * 0.00390625 as y, ---convert to 0.0-1.0
                            z * 0.00390625 as z  ---convert to 0.0-1.0
                       FROM accel
                       WHERE x is not null
                         AND y is not null
                         AND z is not null
                         -- this clause filters out duplicate times
                         AND rowid in (SELECT max(rowid) FROM accel GROUP BY ts)
                       ORDER BY ts, ROWID DESC;
                       """.format(iidstr, refDate)
    selectionsql['sd_gps'] = """
        SELECT '{}' AS iid, 
            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                    (ts/1000000.0)||' seconds') as ts,
                lat, lon, 
                speed, 
                course, 
                -- ignoring mode, fix, 
                -- ignoring mode1, mode2, 
                -- following fields set to NODATA (-9999) if null
                IFNULL(alt,-9999), 
                IFNULL(sat_used,-9999), 
                IFNULL(pdop,-9999), 
                IFNULL(hdop,-9999), 
                IFNULL(vdop,-9999), 
                IFNULL(sat_in_view,-9999) 
            FROM gps
            WHERE   (lat is not null)
                AND (lon is not null)
                AND (lat BETWEEN -90 and 90)
                AND (lon BETWEEN -180 and 180)
                AND (alt is NULL or alt BETWEEN -10000 and 100000)
                AND (course is NULL or course BETWEEN -360 and 360)
                AND (speed is NULL or speed BETWEEN 0 and 1000)
                AND (sat_used is NULL or sat_used BETWEEN 0 and 50)
                AND (sat_in_view is NULL or sat_in_view BETWEEN 0 and 50)
                -- this clause filters out duplicate timestamps 
                AND (rowid in (SELECT max(rowid) FROM gps GROUP BY ts))
            ORDER BY ts, ROWID DESC;
        """.format(iidstr, refDate)
                #-- following fields either null or in legal range
        # NOTE: In the above queries, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of 1000000.0, 
        # to preserve the floating-point nature of the result.
        # The rows are sorted by ts, and then  ROWID DESC because 
        # we need to filter later by the order in which the rows were 
        # written to the file, keeping only the latest instance in cases
        # where records share the same timestamp info.

    if detect:
        # if we're looking for leaks, count rows before ingesting
        rows_before_ingest = count_db_rows('sd_gps') 

    # Now ingest each of the required tables into the DB.
    success = True
    for tbl in selectionsql:
        success = success and execute_copy_via_shell(filepath, 
                                                     city,wave,stream,
                                                     iidstr, tbl, 
                                                     selectionsql[tbl]) 
    if detect:
        # then count rows again after ingesting
        rows_after_ingest = count_db_rows('sd_gps') 
        rows_ingested = rows_after_ingest - rows_before_ingest
        rows_expected = gps_file_row_counts[filepath]
        drop_rate = (rows_expected-float(rows_ingested))/rows_expected
        log("ingested %d vs expected %d = %g loss for '%s'"%(rows_ingested,
                    rows_expected, drop_rate, filepath))
        if drop_rate > 0.7:
            print("Dropped WAY too many records for %s (%g). Emergency stop."%(iidstr,drop_rate))
            exit()
    return success


def ingest_user_files(iidstr, city,wave,stream, filepathlist):
    """
    Given an interact_id string and a list of filepaths, 
    perform basic acceptance tests on the data files and
    if they pass, load them into the database, tagged with the iid.
    """
    log("Ingesting files inventoried for user: %s" % iidstr)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_file_stats = select_usable_files(filepathlist, iidstr)

    if not ingest_file_stats:
        log("Files found for participant %s were not usable."%iidstr)
        return False

    log("User %s has ingestible files."%iidstr)

    # now actually ingest the datafiles
    success = True
    for [min_dt, max_dt, count, sdbfile] in ingest_file_stats:
        result = ingest_sdb_file(sdbfile,city,wave,stream,iidstr)
        success = success and result

    return success


def count_db_rows(tablename):
    """
    Given the name of a table in the Postgres DB, return 
    the row-count from that table.
    """
    # dbURI = 'postgresql://{}@{}:{}/{}'.format(db_user,
    #                                 db_host, db_host_port,
    #                                 db_name)
    # engine = create_engine(dbURI)
    # engine.execute(q)
    count=-1
    with psycopg2.connect(user=db_user,
                          host=db_host,
                          port=db_host_port,
                          database=db_name) as conn:
        c = conn.cursor()
        sql = "SELECT count(1) FROM %s.%s" % (db_schema,tablename)
        c.execute(sql)
        count = int(c.fetchone()[0])
    return count


if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    # if user has specified a log filename, use that instead 
    # of the default
    if args['-L']:
        logfile = args['-L']

    detect = args['-D']

    if args['-E']:
        print("Command line args:")
        pprint(args)
        print("DB_HOST: %s"%db_host)
        print("DB_HOST_PORT: %s"%db_host_port)
        print("DB_USER: %s"%db_user)
        print("DB_NAME: %s"%db_name)
        print("DB_SCHEMA: %s"%db_schema)
        print("Log file: %s"%logfile)
        print("leak detect: %s"%detect)
        exit()

    citynum = -1
    if args['CITYID']:
        citynum = int(args['CITYID'])
    wavenum = -1
    if args['WAVEID']:
        wavenum = int(args['WAVEID'])
    streamnum = -1
    if args['STREAMID']:
        streamnum = int(args['STREAMID'])

    if args['-H']:
        hygiene_dir = args['-H']
        if not os.path.isdir(hygiene_dir):
            print("Invalid hygiene directory: %s. Aborting."%hygiene_dir)
            exit()

    with open(logfile, 'w') as loghandle:
        log("Ingest script was run with following arguments:")
        log(args)

        # process ingest restrictions
        target_iids = []
        if args['-i']:
            target_iids.append(args['-i'])
            print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

        if args['PATH']:
            # Ingesting a single file is quick and easy.
            # (This is used primarily for testing.)
            if os.path.isfile(args['PATH']):
                fpath = args['PATH']
                if fpath.lower().endswith('.sdb'):
                    ingest_sdb_file( [fpath, "007"] )
                else:
                    err("Only .sdb files can be ingested.")
                exit()

        # Otherwise, we ingest directories recursively.
        rootdir = args['PATH']
        log("Root directory: %s" % rootdir)

        # The files we're looking for are organized in directories by
        # the unique pairing of interact_id and sdid. 
        # So there's no point in trying to ingest by directory, because
        # a single participant's SDBs can be found in multiple
        # directories. What makes more sense is to ingest all the 
        # specific sdb files associated with a particular user.
        participant_files = {}
        print("Taking file inventory...")
        for foldername in tqdm(os.listdir(rootdir)):
            folderpath = os.path.join(rootdir, foldername)
            if os.path.isdir(folderpath): # we have a data folder
                log("Inventory D %s" % folderpath)
                sdidstr = ''
                iidstr = foldername
                # sdb data for some participants split over mult folders
                if '_' in foldername:
                    iidstr,dummy,sdidstr = foldername.partition('_')
                # If we're limiting to a specific user, don't
                # both processing other people
                if target_iids and iidstr not in target_iids:
                    mention("Ignoring user %s who is not target."%iidstr)
                    continue
                # all sdb data within folder belongs to given participant
                for filename in os.listdir(folderpath):
                    filepath = os.path.join(folderpath, filename)
                    if filename.lower().endswith('.sdb'):
                        if not iidstr in participant_files:
                            participant_files[iidstr] = []
                        participant_files[iidstr].append(filepath)
            else:
                log("Inventory F %s" % folderpath)

        # Now we have a dictionary with a list of file paths for each
        # participant

        # Before we ingest the files, take a row count from the existing
        # DB tables so that we can count again post-ingest and compare
        # against the number of rows we intended to ingest from each
        # of the SDB files. This will give some measure of confidence
        # as to whether the ingest was successful.
        # In this pre-ingest count, we report both GPS and Accel counts,
        # despite the higher cost of counting accel rows, so we could
        # conceivably use those record counts to rollback if the ingest
        # fails.
        baseline_gps_rows = count_db_rows('sd_gps')
        baseline_accel_rows = count_db_rows('sd_accel')
        log("Baseline GPS Count: %d"%baseline_gps_rows)
        log("Baseline Accel Count: %d"%baseline_accel_rows)

        # Now ingest all the files associated with each participant
        print("Ingesting inventoried files...")
        for iidstr in tqdm(participant_files):
            if participant_files[iidstr]:
                log("Inventory for %s holds: %s" % (iidstr,participant_files[iidstr]), prebreak=True)
                result = ingest_user_files(iidstr,
                                           citynum,wavenum,streamnum,
                                           participant_files[iidstr])
            else:
                log("Inventory ? %s" % iidstr)

        # Count final GPS table size
        final_gps_rows = count_db_rows('sd_gps') 

        # Report before, after, and expected row counts. After the
        # initial baseline, we only track GPS table rows, as they are 
        # much smaller than the accel tables, so they're much faster to 
        # compute and still provide a reasonable metric of success.
        log("Baseline GPS Count: %d"%baseline_gps_rows,
                                    screen=True,prebreak=True)
        log("Expected GPS Count: %d"%(baseline_gps_rows+expected_gps_rows),screen=True)
        log("Complete GPS Count: %d"%final_gps_rows, screen=True)
        ingested_gps_rows = final_gps_rows - baseline_gps_rows
        ingest_rate = 100.0*float(ingested_gps_rows)/float(expected_gps_rows)
        if ingest_rate > 99.5:
            log("Ingest looks successful with %0.2f%% row retention."%ingest_rate, screen=True)
            log("Remember, some minor row loss is expected due to timestamp duplication in SDB files.",screen=True)
        else:
            log("Row retention of %0.2f%% looks low."%ingest_rate, screen=True) 
            log("Minor row loss is expected due to duplicated timestamps in the SDB files, but this seems excessive.", screen=True) 
            log("But if this was a re-ingest of an uncompleted previous ingest cycle, high row loss is to be expected.", screen=True)

