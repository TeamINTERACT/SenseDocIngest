#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

Usage:
  sensedoc_ingest [options] DIRPATH
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -i IID        Ingest participant IID only
    -v,--verbose  Provide more verbose output
"""
import os
import sqlite3
import datetime
import subprocess as sub
from docopt import docopt
from itertools import combinations
from pprint import pprint
# from concurrent.futures import ThreadPoolExecutor as PoolExecutor

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 10000
dbname = 'interact_db'
logfile = datetime.datetime.now().strftime("ingest_%Y%m%d-%H%M%S.log")
loghandle = None

def log(str):
    if loghandle:
        loghandle.write(str + '\n')
    if args['--verbose']:
        print(str)

def mention(str):
    if args['--verbose']:
        print(str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    return not (dt1_end < dt2_st or dt1_st > dt2_end)
    # return not (dt1_st < dt2_end and dt1_end >dt2_st)

def remove_useless_files(fileset, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the directory contains acceptable data, return the 
    list of acceptable files. Otherwise, return None.

    A participant's entire folder is considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So making the assumption that if the gps table contains good
    # dates and data, the accel table will as well. 
    sql = """
            SELECT MIN(utcdate) as mindate,
                    MAX(utcdate) as maxdate,
                    COUNT(*) as count
            FROM gps
            """
    fstats = []
    good_files = []
    for fpath in fileset:
        conn = sqlite3.connect(fpath)
        c = conn.cursor()
        c.execute(sql)
        mindate,maxdate,count = c.fetchone()
        if mindate and maxdate and count:
            fstats.append([mindate,maxdate,count])
            good_files.append(fpath)
        else:
            mention("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                           mindate,
                                                           maxdate,
                                                           count))
    if len(fileset) > 1:
        # compare all possible pairings of files and reject 
        # this participant if any pairs have overlap between 
        # their min and max date stamps.
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                spanstr = "   %s - %s\n   %s - %s"%(stat1[0], stat1[1], 
                                                stat2[0], stat2[1])
                log("Overlapping timestamps found for participant %s:\n%s" % (iidstr,spanstr))
                return None
        # total the number of samples from all files and
        # reject the participant if the number of samples does not
        # meet the minimum threshold
        if sum([x[2] for x in fstats]) < min_sample_threshold:
            log("Not enough samples for participant %s"%iidstr)
            return None
        return good_files
    elif len(fileset) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return good_files

    log("Insufficient usable data for participant %s." % iidstr)
    return None

def execute_copy_via_shell(filepath, tablename, selectionsql, pid):
    # Now we have everything we need to ingest the datafile. 
    # On the command line we could run a piped command to
    # pull data from the sqlite file and ingest it into the postgres
    # DB. By using the sqlite3 and psql clients, which have both
    # been optimized for CSV streaming, we can load the data into the
    # DB much more efficiently than to load it into a Python script
    # and then push it to the DB.
    #
    # In simple form, the console command is:
    # sqlite3 SDBFILE SELECTIONSQL | psql DBNAME COPYCMD
    #
    # where SELECTIONSQL is:
    #       "SELECT 'SOMEVALUE' as iid, 
    #         strftime('%Y-%m-%d %H:%M:%f', 
    #                  '2017-01-01 03:04:05',  ---Some refDate
    #                  (ts/1000000.0)||' seconds') as ts, 
    #         x, y, z 
    #         FROM accel;"
    #
    # and COPYCMD is:
    #       "COPY level_0.accel FROM STDIN delimiter '|' CSV;"
    #
    # That COPYCMD would do a simple ingest, directly into the table,
    # but we also need to scrub out duplicate records, which occur in 
    # the files occasionally due to a minor glitch in the firmware.
    #
    # So, to eliminate duplicates, we'll create a dummy table and load
    # the data into that, and then use INSERT ON CONFLICT DO NOTHING to
    # ignore duplicates as we copy the dummy table to the live table.
    # print("Filepath: %s"%filepath)
    # print("Tablename: %s"%tablename)
    # print("PID: %s"%pid)
    # tableid = '_%s_%s' % (tablename, pid)
    tableid = ''
    copycmd = """
             CREATE TABLE level_0.deleteme{tid} (LIKE level_0.{tn}); 
             COPY level_0.deleteme{tid} FROM STDIN delimiter '|' CSV;
             INSERT INTO level_0.{tn}
                SELECT * FROM level_0.deleteme{tid}
                ON CONFLICT (iid,ts) DO NOTHING;
             DROP TABLE level_0.deleteme{tid};
             """.format(tn=tablename,tid=tableid)

    cmdline = 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath,
                                                        selectionsql,
                                                        dbname,
                                                        copycmd)
    mention("Command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        log("Ingesting file %s produced return code: %s"%(filepath,res))
        return False
    return True

def ingest_sdb_file(filepath, iidstr):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    refDate = ''
    with sqlite3.connect(filepath) as conn:
        # get the reference date from the SDB from which all 
        # timestamps are measured
        c = conn.cursor()
        c.execute("SELECT value FROM ancillary WHERE key='refDate'")
        refDate = c.fetchone()[0]

    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

LOOK AT THE GITLAB CODE TO BE SURE ALL DEFAULTS AND ADJUSTMENTS ARE MADE

    selectionsql = {'accel':"""
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---scale factor 
                            y * 0.00390625 as y, ---scale factor
                            z * 0.00390625 as z  ---scale factor
                       FROM accel;
                       """.format(iidstr, refDate),
                    'gps':"""
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            lat, lon, speed, course, mode, fix, alt, 
                            mode1, mode2, sat_used, 
                            pdop, hdop, vdop, sat_in_view 
                            FROM gps;
                        """.format(iidstr, refDate)
        }
        # NOTE: In the above SQL code, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of the 
        # divisor constant, to preserve the floating-point nature 
        # of the result.

    # Now ingest each of the required tables into the DB
    # If any table fails, the ingest for that participant should be 
    # reported as unsuccessful.
    success = True
    argsets = []
    for i,tbl in enumerate(selectionsql):
        argsets.append([filepath, tbl, selectionsql[tbl], i])
    # print("Argsets:")
    # pprint(argsets)

    # now create multiple threads and dispatch them to run 
    # execute_copy_via_shell() with each argset
    # with PoolExecutor(max_workers=4) as executor:
        # executor.map(execute_copy_via_shell, argsets)
    for argset in argsets:
        success = success and execute_copy_via_shell(*argset)
    return success


def ingest_user_directory(rootdir, iidstr):
    """
    Given an interact_id number in string form and a path to a data dir,
    perform basic acceptance tests on the data files in that dir and
    if they pass, load them into the database, tagged with the iid.
    """
    # if not iidstr.endswith('90'):
    #     return
    dirpath = os.path.join(rootdir, iidstr)
    mention("Attempting ingest on directory: %s" % dirpath)

    # get a list of datafiles in this directory
    candidate_files = []
    for fname in os.listdir(dirpath):
        filepath = os.path.join(dirpath, fname)
        if fname.endswith('.sdb') and iidstr in filepath:
            # then this is a sensedoc datafile
            candidate_files.append(filepath)
            mention("Adding target file: %s" % filepath)
        else:
            mention("Skipping target file: %s" % filepath)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_files = remove_useless_files(candidate_files, iidstr)

    if not ingest_files:
        log("No usable files found for participant %s"%iidstr)
        return False

    mention("Directory contains ingestible files.")

    # now actually ingest the datafiles
    success = True
    for sdbfile in ingest_files:
        success = success and ingest_sdb_file(filepath, iidstr)
    
    return success

if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')
    log(args)

    # process ingest restrictions
    target_iids = []
    if args['-i']:
        target_iids.append(args['-i'])
        print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

    rootdir = '.'
    if args['DIRPATH']:
        if not os.path.isdir(args['DIRPATH']):
            err("Path %s is not a valid directory."%args['DIRPATH'])
        rootdir = args['DIRPATH']
    log("Root directory: %s" % rootdir)

    # The files we're looking for are organized in directories by
    # interact_id. The directory name is the participant's iid,
    # and the file within that directory is {iid}.sdb
    with open(logfile, 'w') as loghandle:
        for dirpath in os.listdir(rootdir):
            if not target_iids or dirpath in target_iids:
                if ingest_user_directory(rootdir, dirpath):
                    log("Participant %s ingested successfully."%dirpath)
                else:
                    log("Problem ingesting participant %s"%dirpath)

