#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script verifies that an archived set of city/wave data folders
have the expected participant folders inside.

The expected values are computed from the coordinator's supplied 
linkage file, which lists the SDID values assigned to each participant
IID.

By reading the assignments from the CSV, we can construct the
expected directory names and then look for them in the target directory.
They'll be in the form: {IID}_{SDID}

Any folders not found are reported in the log file.

We can also do the reverse: look at all the directories in the folder
and report on any that do not match an IID_SDID pair. This will help
us identify cases where data is mislabeled, rather than actually missing.

Usage:
  linkage_verify [options] CSVFILE ROOTPATH
  linkage_verify -h | --help | -V | --version

Options:
    -h            Display this help info
    -L FNAME      Save log to FNAME
    -t ID         Produce trace log of all decisions about user #ID
    -v,--verbose  Provide more verbose output
"""

import os
import re
import csv
import datetime
from docopt import docopt
from tqdm import tqdm

loghandle = None
logfile = datetime.datetime.now().strftime("linkage_verify_%Y%m%d-%H%M%S.log")

def log(msg, prebreak=False, screen=False):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s\n" % msg)
        loghandle.flush()
    if screen:
        if prebreak:
            print('')
        print("LOG: %s" % msg)


if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    # if user has specified a log filename, use that instead 
    # of the default
    if args['-L']:
        logfile = args['-L']

    traceiid = ''
    if args['-t']:
        traceiid = args['-t']

    def trace(curiid, msg):
        if traceiid and curiid == traceiid:
            print("TRACE %s: %s"%(traceiid,msg))

    linkage_filename = args['CSVFILE']
    root_dir = args['ROOTPATH']

    # 'interact_id'
    # 'sensedoc_serial'
    # 'data_disposition'

    expected_dirs = []
    ignoring_dirs = []
    iids_with_missing_dates = []
    with open(logfile, 'w') as loghandle:
        # CSV file has these column headers
        # MONTREAL WAVE 1 HEADER
        #    INTERACT ID;uid;Ethica truth;SD number;SD start ;SD end;
        #    Sd number 2 ;SD 2 start ;SD 2 end;Dropout
        # VICTORIA WAVE 1 HEADER
        #    INTERACT ID,treksoft_id,Sensedoc participant ID,
        #    Sensedoc ID,Dates worn,Ethica ID,,,,,,Notes
        with open(linkage_filename,'r',encoding='ISO-8859-1') as fcsv:
            # sdpat = '^(?P<sdid>\d+)-(?P<fw>\d+)'
            reader = csv.DictReader(fcsv,delimiter=',')
            # shown = False
            for rownum,row in enumerate(list(reader)):
                # print(row.keys())
                iid = row['interact_id']
                if iid: 
                    # sdids = row['Sensedoc ID']
                    # this is technically the serial number, not an id
                    sdids = row['sensedoc_serial']
                    if '/' in sdids:
                        log("Participant %s has multiple serial values (%s) in one record."%(iid,sdids))
                        log("WARNING: Validation will continue, but linkage file cannot be ingested until this entry has been split into multiple rows.")
                        trace(iid,"Slash found in sdid")
                    else:
                        trace(iid,"No slash found in sdid")


                    for sdfw in sdids.split('/'):
                        sdid = None
                        if '-' in sdfw: # Mtl encoded them one way
                            sdid,dum,fw = sdfw.strip().partition('-')
                            trace(iid,"SDID encoded with -")
                        elif '_' in sdfw: # Ssk encoded them differently
                            dum,dum,sdid = sdfw.strip().partition('_')
                            trace(iid,"SDID encoded with _")
                        else:
                            sdid = sdfw
                            trace(iid,"SDID not encoded with - or _")
                        if not sdid or sdid.lower() == 'na':
                            trace(iid,"SDID not included")
                        elif all(x in '0123456789' for x in sdid):
                            sdid = str(int(sdid))
                            trace(iid,"SDID is %s"%sdid)
                            # if row['data_disposition'] == 'ingest':
                            if 'data_disposition' in row.keys() \
                            and 'ignore' in row['data_disposition']:
                                trace(iid,"record marked 'ignore' in linkage")
                                ignoring_dirs.append("%s_%s"%(iid,sdid))
                                log("Ignoring folder for %s_%s: %s"%(iid,sdid,row['data_disposition']))
                            else:
                                foldername = "%s_%s"%(iid,sdid)
                                trace(iid,"record not marked 'ignore' in linkage, so data folder %s is expected"%foldername)
                                if not row['start_date'] or not row['end_date']:
                                    log("INCOMPLETE WEAR-DATE RECORD: %s"%foldername)
                                    iids_with_missing_dates.append(foldername)
                                expected_dirs.append(foldername)
                        else:
                            msg = "Participant %s has illegal SD_ID: '%s'"%(iid,sdid)
                            trace(iid,msg)
                            log(msg)
                else:
                    log("CSV row #%d has no iid"%rownum)

        log("Expecting %d directories."%len(expected_dirs))
        # print('\n'.join(expected_dirs))

        count_found_dirs = 0
        count_unfound_dirs = 0
        for d in expected_dirs:
            path = os.path.join(root_dir, d)
            if os.path.isdir(path):
                count_found_dirs += 1
                # Now look in that expected dir and validate SDB fname
                fnpat = "SD\d+fw\d+_\d+_\d+[^.]*\.[sS][dD][bB]"
                for fn in os.listdir(path):
                    if fn.lower().endswith('.sdb'):
                        prob = []
                        m = re.search(fnpat,fn)
                        if not m:
                            log("  BAD SDB FNAME: %s"%os.path.join(path,fn))
            else:
                log("  EXPECTED DIRECTORY MISSING: '%s'" % path)
                count_unfound_dirs += 1

        # now look for unexpected directories
        count_unexpected_dirs = 0
        for child in os.listdir(root_dir):
            if not child in expected_dirs and not child in ignoring_dirs:
                for childf in os.listdir(os.path.join(root_dir,child)):
                    if childf.endswith('.sdb'):
                        log("  DIRECTORY IS UNEXPECTED: '%s'" % child)
                        count_unexpected_dirs += 1
                        break

        log("Found %d of the %d expected directories." % (count_found_dirs,len(expected_dirs)), screen=True)
        log("%d are missing." % count_unfound_dirs, screen=True)
        log("Found %d unexpected directories." % count_unexpected_dirs, screen=True)
        log("Found %d data folders without wear-date records."%len(iids_with_missing_dates), screen=True)


# Known problems that we can safely ignore
# Expecting 163 directories.
# UNABLE TO FIND DIRECTORY: '/home/jeffs/projects/def-dfuller/interact/permanent_archive/Victoria/Wave1/SenseDoc/101158091_23'
    # DEVICE damaged, no data extracted for this pairing
# UNABLE TO FIND DIRECTORY: '/home/jeffs/projects/def-dfuller/interact/permanent_archive/Victoria/Wave1/SenseDoc/101891218_111'
    # DEVICE FAILED: No data extracted for this pairing
# DIRECTORY IS UNEXPECTED: '101996732_147'
    # This user has data folders for device 60 and 147
    # Both data folders have plenty of data
    # Will need to investigate why script is claiming no data found
    # It is missing from the linkage file
    # He wore 147 from Sep 19 - 24
    # He wore  60 from Sep 11-16
# DIRECTORY IS UNEXPECTED: '101962906_111'
    # File contains about 2.5 hrs of data from Oct 11, 2017
    # Karen advises that he switched to Ethica and that this
    # data file should be ignored and/or filtered out.
# Found 161 of the expected directories.
# 2 of the expected directories are missing.
# Found 2 unexpected directories.
