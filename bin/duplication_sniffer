#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree and looks in every SDB file it finds,
counting the number of duplicated timestamps.

The current ingest process has reported a seemingly high number of dropped rows for Montreal (1.1%) and I want to see whether that can be attributed to duplicate rows.

Hmm. Duplicates and illegal values account for 0.0024% drop in Victoria and 0.0028% in Montreal. The 1.1% must be coming from some other cause.

Usage:
  duplication_sniffer [options] ROOTPATH
  duplication_sniffer -h | --help | -V | --version

Options:
    -h            Display this help info
    -L FNAME      Save log to FNAME
    -i IID        Examine participant IID only
    -v,--verbose  Provide more verbose output
"""

import os
# import re
# import csv
import sqlite3
import datetime
# import subprocess as sub
from docopt import docopt
from tqdm import tqdm

target_iid = None
loghandle = None
logfile = datetime.datetime.now().strftime("duplication_report.log")

def log(msg, prebreak=False, screen=False):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s\n" % msg)
        loghandle.flush()
    if screen:
        if prebreak:
            print('')
        print("LOG: %s" % msg)

def mention(str):
    if args['--verbose']:
        print(str)
        # verbose statements written to the log as well as the screen
        # if loghandle:
            # loghandle.write("%s\n"%str)

def err(str):
    log("ERR: %s"%str)
    exit(1)

def is_iid(instr):
    return all([x in '0123456789' for x in instr])

def part_dir_test(dirname, iid):
    # part dirs start with the iid and end with things like
    # -part1 -part2 or device1 device3, etc
    pat = "\w+\d$"
    return dirname.startswith(iid) and re.search(pat,dirname)
            
def find_sdid_for_path(dirpath):
    # the sdid is encoded into the fname of the SDB file
    sdid = ''
    pat = 'SD(?P<SDID>\d+)\w+\d+_\d+_\d+.sdb'
    for fname in os.listdir(dirpath):
        m = re.match(pat, fname)
        if m:
            sdid = m.group('SDID')
            break
    if not sdid:
        # some directories do not include the sdid in the SDB
        # filename and instead have a file called SD00{SDID}.SD2
        pat = 'SD0*(?P<SDID>\d+).SD2'
        for fname in os.listdir(dirpath):
            m = re.match(pat, fname)
            if m:
                sdid = m.group('SDID')
                break
    if not sdid:
        log("No SDID found in: ...%s" % dirpath[-20:])
    return sdid 

def find_parts_dirs(dirpath, iid):
    # traverse dirpath and look for subdirectories
    # that match the naming conventions for part directories
    subs = []
    for child in os.listdir(dirpath):
        if part_dir_test(child, iid):
            subs.append(child)
    return subs

def list_match(instr, strlist):
    """
    Find the first string from strlist that is contained in instr.
    Return the matching strlist element.
    """
    for pat in strlist:
        if pat in instr:
            return pat
    return None

def count_duplicate_timestamps(fpath):
    """
    Given an SDB filepath, query its GPS table and count the number
    of records that will be dropped due to redundant timestamps.
    """
    ill_count = 0
    dup_count = 0
    total_count = 0
    sql_tot = "SELECT count(1) AS n FROM gps;"
    sql_dups = "SELECT ts,count(1) AS n FROM gps GROUP BY ts HAVING n>1;"
    sql_illegals = """
        SELECT count(1) FROM gps 
        WHERE lat is null
           OR lon is null
           OR lat NOT BETWEEN -90 and 90
           OR lon NOT BETWEEN -180 and 180
           OR alt NOT BETWEEN -10000 and 100000
           OR course NOT BETWEEN -360 and 360
           OR speed NOT BETWEEN 0 and 1000
           OR sat_used NOT BETWEEN 0 and 50 
           OR sat_in_view NOT BETWEEN 0 and 50;
        """
    mention("Counting in SDB file: %s"% fpath)
    with sqlite3.connect(fpath) as conn:
        c = conn.cursor()

        # count total rows in file
        c.execute(sql_tot)
        total_count = int(c.fetchone()[0])

        # count duplicates in file
        c.execute(sql_dups)
        for row in c.fetchall():
            rowcount = int(row[1])
            # one of each set of duplicated records will be kept
            # so the number dropped will be one less
            dup_count += rowcount-1

        # count illegal rows in file
        c.execute(sql_illegals)
        ill_count = int(c.fetchone()[0])
    return dup_count, ill_count, total_count


if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    # if user has specified a log filename, use that instead 
    # of the default
    if args['-L']:
        logfile = args['-L']

    if args['-i']:
        target_iid = args['-i']

    root_path = args['ROOTPATH']

    count_sdb_files = 0
    for headdir,dirs,files in os.walk(root_path, followlinks=True):
        for fname in files:
            if fname.lower().endswith(".sdb"):
                count_sdb_files += 1

    total_rows = 0
    total_dups = 0
    total_ills = 0
    total_dropped = 0
    with open(logfile, 'w') as loghandle:
        # Now traverse the source directories and look for the
        # device-specific subdirectories that need to be promoted to 
        # directories in their own right.
        # Note: The term 'bad' in a varname means that the content
        #       still references the bad iids.
        with tqdm(total=count_sdb_files) as pbar:
            for headdir,dirs,files in os.walk(root_path, followlinks=True):
                for fname in files:
                    fpath = os.path.join(headdir, fname)
                    # skip non-target files if target_iid has been given
                    if target_iid and not target_iid in fpath:
                        continue
                    if fname.lower().endswith(".sdb"):
                        dups,ills,tot = count_duplicate_timestamps(fpath)
                        log("%d, %d, %s" % (dups, tot, fpath))
                        total_dups += dups
                        total_rows += tot
                        total_ills += ills
                        total_dropped += ills + dups
                        pbar.update(1)
        log("RESULT OF SEARCH FOR ILLEGAL ROWS")
        log("%d, TOTAL ROWS" % total_rows, screen=True) 
        log("%d, TOTAL DUPS" % total_dups, screen=True) 
        log("%d, TOTAL ILLS" % total_ills, screen=True) 
        log("%d, TOTAL DROPPED" % total_dropped, screen=True) 
        log("%0.5f%%  TOTAL DROP RATE" % (100.0*float(total_dropped)/total_rows), screen=True)
