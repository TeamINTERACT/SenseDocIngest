#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script walks a directory tree to find all the SenseDoc .sdb 
files matching a particular filename pattern and ingests them into 
a PostgreSQL database.

The status of each participant directory is logged to:
    ./ingest-YYYYMMDD-HHMMSS.log

NOTE:
This code has been instrumented to run multithreaded over the I/O
sections, but to make use of that, we need to run the job wrapped in
a bash shell that sets up the SLURM settings. Otherwise no additional
threads or processors will be available in the runtime environment.

NOTE:
After discussion with ComputeCanada IT, they discourage running this kind of job in parallel because opening and closing lots of smaller files actually degrades performance, given the underlying structures used for the file system.

Usage:
  sensedoc_ingest [options] PATH CITYID WAVEID 
  sensedoc_ingest -h | --help | -V | --version

Options:
    -h            Display this help info
    -H DIR        Run data hygiene scripts from DIR on incoming data
    -L FNAME      Save log to FNAME
    -D            Detect row leakage
    -i IID        Ingest participant IID only
    -e FNAME      Load a list of files to skip with known problems 
    -E            Display runtime environment vars and then quit 
    -v            Provide more verbose output
"""
import os
import re
import csv
import sqlite3
import datetime
import psycopg2
import subprocess as sub
from tqdm import tqdm
from docopt import docopt
from itertools import combinations
from pprint import pprint
from concurrent.futures import ThreadPoolExecutor as PoolExecutor

try:
    #db_name = 'interact_db'
    db_host = os.environ["SQL_LOCAL_SERVER"]
    db_host_port = int(os.environ["SQL_LOCAL_PORT"])
    db_user = os.environ["SQL_USER"]
    db_name = os.environ["SQL_DB"]
    db_schema = os.environ["SQL_SCHEMA"]
except KeyError as err:
    print("A required runtime environment variable was not found.")
    print(err)
    print("Have you set up the run-time secrets?")
    exit(1)

hygiene_dir = ''

# number of GPS samples a participant must have across all their
# data files to be included in the database
min_sample_threshold = 3600 # equates to 1 hour of usable data
detect = True
purge = False
logfile = datetime.datetime.now().strftime("tmp_ingest_%Y%m%d-%H%M%S.log")
loghandle = None

# a stat we can track to give confidence that the ingest worked
expected_gps_rows = 0
gps_file_row_counts = {}

warnings_issued = False
def log(msg, prebreak=False, prefix='LOG'):
    if loghandle:
        if prebreak:
            loghandle.write("\n")
        loghandle.write("%s: %s\n" % (prefix,msg))
        loghandle.flush()

def mention(str):
    if args['-v']:
        log(str, prefix="DBG")

def warn(str):
    warnings_issued = True
    log(str, prefix="WRN")

def err(str):
    log(str,prefix="ERR")
    print("ERROR: %s"%str)
    exit(1)

def test_overlapping_daterange(dt1_st, dt1_end, dt2_st, dt2_end):
    """
    Given the start and end of two different date ranges, determine
    whether the ranges overlap.
    """
    # Make sure each start and end pair are in sorted order
    start1 = min(dt1_st, dt1_end)
    end1 = max(dt1_st, dt1_end)
    start2 = min(dt2_st, dt2_end)
    end2 = max(dt2_st, dt2_end)

    # If one range occurs entirely before the other, there is no overlap.
    # Otherwise there is.
    return not (end1 < start2 or start1 > end2)

def select_usable_files(filelist, iidstr):
    """
    Given a list of SDB filenames for a single participant,
    perform basic acceptability tests on their contents.
    If the set contains acceptable data, return a list of them. 
    Otherwise, return None.

    The entire set should be considered unacceptable if:
        - the timespans of two or more datafiles overlap
        - the total number of samples across all the files < threshold
    A specific file is removed from consideration if it contains no
    usable data.
    """

    mention("Examining files for participant: %s" % iidstr)
    # pprint(filelist)
    # Accel tables are MUCH denser and take a lot longer to scan.
    # So to speed up the process, I'm assuming that if the gps table 
    # contains good dates and data, the accel table will as well. 
    sql_min = "SELECT utcdate from gps order by ts ASC limit 1"
    sql_max = "SELECT utcdate from gps order by ts DESC limit 1"
    sql_n   = "SELECT count(1) as n from gps"

    fstats = []
    for fpath in filelist:
        # open the file and compute its date range and record count
        try:
            # log("Connecting to source file %s"% fpath)
            with sqlite3.connect(fpath) as conn:
                c = conn.cursor()

                mindate = 0
                c.execute(sql_min)
                row = c.fetchone()
                if row:
                    mindate = row[0]

                maxdate = 0
                c.execute(sql_max)
                row = c.fetchone()
                if row:
                    maxdate = row[0]

                count = 0
                c.execute(sql_n)
                row = c.fetchone()
                if row:
                    count = int(row[0])

                # hang onto these counts for final stats
                # after the ingest is finished
                gps_file_row_counts[fpath] = count
                

        except Exception as e:
            log("Caught a SQL error while counting records.")
            log(e)

        # make a note of those stats if they're usable
        if mindate and maxdate and count:
            mention("Keeping file %s"% fpath)
            fstats.append([mindate,maxdate,count,fpath])
        else: # otherwise leave this file out of the list
            log("File %s unacceptable (%s, %s, %s)" % (fpath,
                                                        mindate,
                                                        maxdate,
                                                        count))
    if len(fstats) > 1:
        # compare all possible pairings of files and reject 
        # this filelist if any pair has overlap between 
        # their min and max date stamps.
        mention("Testing overlaps for %s"% iidstr)
        for stat1,stat2 in combinations(fstats,2):
            if test_overlapping_daterange(stat1[0], stat1[1], 
                                          stat2[0], stat2[1]):
                log("Overlapping timestamps for %s found between files:"%iidstr)
                log("  %s" % (stat1[3]))
                log("    %s -> %s"%(stat1[0], stat1[1])) 
                log("    %s -> %s"%(stat2[0], stat2[1])) 
                log("  %s" % (stat2[3]))
                return None
            else:
                mention("Overlap clean.")
        # total the number of samples from all files and
        # reject the filelist if the number of samples does not
        # meet the minimum threshold

        # COULD THIS BE WHERE THE HIGH NUM OF DROPPED ROWS COMING FROM?
        # No, be, because files that get dropped here never get sent
        # to the ingestor, so their counts not added to expected_gps_rows

        contribution = sum([x[2] for x in fstats])
        if contribution < min_sample_threshold:
            log("Not enough samples from participant %s (%d)"%(iidstr,contribution))
            return None
        return fstats
    elif len(fstats) == 1:
        # if there's only one file, there can't be any overlap
        # so just look at record count
        if fstats[0][2] >= min_sample_threshold:
            return fstats

    log("No usable data files from participant %s." % iidstr)
    return None

def indent_block(instr):
    return '\n'.join(['  '+x for x in instr.strip().split('\n')])

def run_data_hygiene_scripts(city,wave,stream,iid,tablename,serial,scriptdir):
    """
    The raw data being ingested is often messy. It contains
    occasional duplicate records, records with illegal values,
    corrupted entries, mislabeled entries, etc.

    These data problems come in two broad categories: some are
    common across all the incoming files, while others are
    specific to particular files, or only occur occasionally
    under very specific conditions. Because of this, we need a
    generic, flexible way to extend the ingest process so that it
    can respond to arbitrarily complex situations.

    To that end, I'm implementing a system of external scripts.
    Any scripts found in the scriptdir directory will be run
    against the incoming data table so that it can work its
    magic. The scripts are expected to be named in the form
    NNN-some-description.* so that we can control the order of
    execution by changing the 3-digit NNN prefix.

    For maximum flexibility, each script will be required to
    handle its own connection to the DB. Any output generated by
    the script will be entered into the runtime log of this
    parent ingest script. The script is expected to return 0 if
    it runs successfully, 1 if it fails, and 2 if it succeeds but
    has determined that the table cannot be ingested. (Which
    means we can stop running additional scripts against it.)
    """

    # for each script in the script directory
    success_code_all = 0
    if scriptdir:
        for root,dirs,files in os.walk(scriptdir):
            # prep the parameters to the script
            # run the script
            for script in sorted(files):
                # if it's a prioritized script file
                if re.match('\d\d-', script):
                    scriptpath = os.path.abspath(os.path.join(root,script))
                    cmd = [scriptpath, iid, 
                           str(city), str(wave), 
                           tablename, serial, '-e']
                    # pprint(cmd)
                    cp = sub.run(cmd, stdout=sub.PIPE)
                    success_code_all = max([success_code_all,
                                            cp.returncode])
                    # log script output and make it easy to 
                    # parse visually in log file
                    log("BEGIN SCRIPT LOG: %s on %s"%(script,tablename))
                    log(indent_block(cp.stdout.decode('utf-8')))
                    log("END SCRIPT LOG: %s"%script)
                else:
                    mention("%s is not a script file."%script)
    return(success_code_all) # return successful completion

def execute_copy_via_shell(filepath, city, wave, stream, iid, 
                           serial, tableid, 
                           wear_start, wear_end,
                           tablename, selectionsql):
    """
    On the command line we could run a single command to
    pull data from the sqlite file with sqlite3 and ingest into the 
    postgres by piping the output directly to psql.

    In simple form, that console command would be:
    sqlite3 somefile.sdb SELECTIONSQL | psql interact_db INSERTSQL

    In the simple form of the command line shown above, the two
    key elements are SELECTIONSQL, which is the SQL fragment used to
    select a table's content out of the target SQLite file, and the
    INSERTSQL, which is the SQL fragment used to load the incoming
    data into the PostgreSQL database.
    
    Unfortunately, the data is known to contain occasional duplicate 
    records, which happen when the SD device syncs its onboard clock 
    with the real world. In these cases, the most accurate data for 
    that timestamp are the values associated with the last record
    recorded, not the first.

    To drop those redundant timestamps, we have to wait until after
    we've ingested the file, because the /COPY command does not 
    permit ON CONFLICT or DISTINCT clauses.

    Furthermore, we can't use ON CONFLICT to drop dups in the main
    table because the main uses a brin index which can't test for
    uniqueness.

    So we load the entire SDB file into a temporary table.
    Then we copy to a second temp table that has a btree index.
    At the same time, we drop any records outside the wear window.
    Then we finally absorb that clean incoming table into the main table. 
    """
    
    # Assemble the SQL commands
    full_tablename = "%s.%s" % (db_schema, tablename)
    raw_tablename = "tmpA%s" % (tableid)
    filtered_tablename = "tmpB%s" % (tableid)
    idx_name = "tmpB%s_btree_idx" % (tableid)
    ingestcmd = """
             SET SCHEMA '{sch}';
             CREATE TABLE {rawtbl} (LIKE {maintbl});
             CREATE TABLE {filteredtbl} (LIKE {maintbl});
             ALTER TABLE {filteredtbl} ADD CONSTRAINT {idx} UNIQUE (iid,ts);

             -- load the raw table
             COPY {rawtbl} FROM STDIN delimiter '|' CSV;

             -- copy the raw table to the nodups tbl 
             -- while dropping records outside wear window 
             -- and dropping duplicate records as well
             INSERT INTO {filteredtbl}
                SELECT * FROM {rawtbl}
                WHERE ts >= '{start}' 
                AND   ts <= '{end}'
             ON CONFLICT (iid,ts) DO NOTHING;
             """.format(maintbl=tablename, rawtbl=raw_tablename,
                        filteredtbl=filtered_tablename,
                        start=wear_start, end=wear_end,
                        idx=idx_name, sch=db_schema)
    absorbcmd = """
             SET SCHEMA '{sch}';
             INSERT INTO {maintbl}
                SELECT * FROM {filteredtbl};
             DROP TABLE {rawtbl};
             DROP TABLE {filteredtbl};
             """.format(maintbl=tablename, rawtbl=raw_tablename,
                        filteredtbl=filtered_tablename,
                        idx=idx_name, sch=db_schema)


    # NOTE: There's still a minor issue here. We tend to want to ingest
    # an entire wave at a time, but if something goes wrong partway 
    # through, we don't have an elegant way to rollback the ingest and 
    # start it again. As it stands, running an ingest a second time 
    # should work fine, since the INSERT ON CONFLICT will simply ignore
    # records that were already ingested. This will admittedly take up 
    # more time, loading all the previously ingested participants 
    # only to ignore their records at copy time. But processing the 
    # entire wave and ignoring records we already have seems the more
    # robust solution as it is less likely to accidentally omit data
    # by trying to be clever about skipping redundant ingests.

    # Tell sqlite where the temp directory is
    cmdline = 'SQLITE_TMPDIR=~/scratch '

    # Load the raw data into a temp table
    cmdline += 'sqlite3 %s "%s" | psql %s -q -c "%s"' % (filepath, 
                                                        selectionsql,
                                                        db_name, 
                                                        ingestcmd)
    mention("Load command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    # res = sub.check_output(cmdline, shell=True,stderr=sub.STDOUT)
    # res = res.decode('utf-8').strip()
    if res:
        # If table load fails, the ingest for that participant 
        # should be reported as unsuccessful.
        log("Ingesting file %s failed with return code: %s"%(filepath,res))
        print("ERR: %s"%res)
        return False

    if detect:
        raw_rows = count_db_rows(raw_tablename)
        filtered_rows = count_db_rows(filtered_tablename)
        rate = 0.0
        if raw_rows:
            rate =  float(filtered_rows)/raw_rows 
        log("STATS: IID={}  SERIAL={}  TABLE={}  RAWROWS={:,} INGESTEDROWS={:,} RATE={:.1%}".format(iid, serial, tablename, raw_rows, filtered_rows, rate))

    if False:
        # Run the hygiene scripts on the temp table
        res = run_data_hygiene_scripts(city,wave,stream,iid,
                                    raw_tablename, serial, hygiene_dir)
        if res:
            # If hygiene fails, the ingest for that participant should be 
            # reported as unsuccessful.
            log("Ingesting file %s failed hygiene tests with return code: %s"%(filepath,res))
            return False
        
    # If the hygiene scripts succeeded, transfer temp table into main
    cmdline = 'psql %s -q -c "%s"' % (db_name, absorbcmd)
    mention("Absorb command line: %s" % cmdline)
    res = sub.call(cmdline, shell=True)
    if res:
        # If any table fails, the ingest for that participant should 
        # be reported as unsuccessful.
        log("Absorbing file %s failed with return code: %s"%(filepath,res))
        return False

    # OTHERWISE, SHOULDN'T WE DROP THE TEMP TABLE?

    return True

def create_table_prefix(fname, iidstr):
    """
    Given a filename and an iid, combine them to create a compact
    name for a temp table.
    """
    prefix = "%s_" % iidstr
    prefix += ''.join([x for x in fname if x.isalnum()])

    return prefix


def ingest_sdb_file(filepath,city,wave,stream,iidstr):
    """
    Given an SDB datafile, load its tables into the database,
    tagged with the iid of the participant who produced it.
    Return True if file ingested properly, False otherwise
    """
    refDate = ''
    # We counted file rows when we were validating the files, 
    # so now that we know this file is being ingested, add its
    # row count to the total number of rows we're trying to ingest.
    global expected_gps_rows
    if filepath in gps_file_row_counts:
        expected_gps_rows += gps_file_row_counts[filepath]
    else:
        log("SDB FILE '%s' was never row-counted.")

    log("Ingesting file: %s"%os.path.basename(filepath))
    try:
        with sqlite3.connect(filepath) as conn:
            # get the reference date from the SDB from which all 
            # timestamps are measured
            c = conn.cursor()
            sql = "SELECT value FROM ancillary WHERE key='refDate'"
            mention("Getting refDate with: %s"%sql)
            c.execute(sql)
            refDate = c.fetchone()[0]
            mention("Got refDate of: %s"%refDate)
    except Exception as e:
        log("Caught a SQL error while ingesting %s"%filepath)
        log(e)


    if not refDate:
        log("Unable to get refDate from file %s"%filepath)
        return False

    # Each table in the SQLite file needs its own SELECT stmt
    # to ensure data is read from the file in a known format.
    selectionsql = {}
    selectionsql['sd_accel'] = """
                       SELECT '{}' AS iid, 
                            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                                    (ts/1000000.0)||' seconds') as ts,
                            x * 0.00390625 as x, ---convert to 0.0-1.0
                            y * 0.00390625 as y, ---convert to 0.0-1.0
                            z * 0.00390625 as z  ---convert to 0.0-1.0
                       FROM accel
                       WHERE x is not null
                         AND y is not null
                         AND z is not null
                         -- this clause filters out duplicate times
                         AND rowid in (SELECT max(rowid) FROM accel GROUP BY ts)
                       ORDER BY ts, ROWID DESC;
                       """.format(iidstr, refDate)
    selectionsql['sd_gps'] = """
        SELECT '{}' AS iid, 
            strftime('%Y-%m-%d %H:%M:%f', '{}', 
                    (ts/1000000.0)||' seconds') as ts,
                lat, lon, 
                speed, 
                course, 
                -- ignoring mode, fix, 
                -- ignoring mode1, mode2, 
                -- following fields set to NODATA (-9999) if null
                IFNULL(alt,-9999), 
                IFNULL(sat_used,-9999), 
                IFNULL(pdop,-9999), 
                IFNULL(hdop,-9999), 
                IFNULL(vdop,-9999), 
                IFNULL(sat_in_view,-9999) 
            FROM gps
            WHERE   (lat is not null)
                AND (lon is not null)
                AND (lat BETWEEN -90 and 90)
                AND (lon BETWEEN -180 and 180)
                AND (alt is NULL or alt BETWEEN -10000 and 100000)
                AND (course is NULL or course BETWEEN -360 and 360)
                AND (speed is NULL or speed BETWEEN 0 and 1000)
                AND (sat_used is NULL or sat_used BETWEEN 0 and 50)
                AND (sat_in_view is NULL or sat_in_view BETWEEN 0 and 50)
                -- this clause filters out duplicate timestamps 
                AND (rowid in (SELECT max(rowid) FROM gps GROUP BY ts))
            ORDER BY ts, ROWID DESC;
        """.format(iidstr, refDate)
                #-- following fields either null or in legal range
        # NOTE: In the above queries, strftime() is used instead of 
        # datetime() because datetime() truncates to seconds, whereas
        # the strftime() with %f keeps fractional seconds. 
        # Also note that the .0 is important at the end of 1000000.0, 
        # to preserve the floating-point nature of the result.
        # The rows are sorted by ts, and then  ROWID DESC because 
        # we need to filter later by the order in which the rows were 
        # written to the file, keeping only the latest instance in cases
        # where records share the same timestamp info.

    if False and detect:
        # if we're looking for leaks, count rows before ingesting
        gps_rows_before_ingest = count_db_rows('sd_gps') 
        accel_rows_before_ingest = count_db_rows('sd_accel') 

    # based on the name of the file being processed, construct a
    # temporary tablename we can use for loading this data.
    # In some cases, we may want to leave the table around after 
    # ingest (for forensic purposes) and if we want that to work
    # each table has to have a unique name.
    # tail = ''
    serial = ''
    basename = os.path.basename(filepath)
    tableprefix = create_table_prefix(basename, iidstr)
    serialpat = "SD(?P<num>\d*)fw(?P<rev>\d*)_(?P<tail>.*)\.sdb"
    m = re.match(serialpat, basename)
    if m:
        if not m.group('num') or not m.group('rev'):
            log("Malformed SDB filename: %s. Does not contain device serial number" % basename)
            return False
        serial = "%s-%s" % (m.group('num'), m.group('rev'))
    if False:
        serialpat = "SD(?P<num>\d*)fw(?P<rev>\d*)_(?P<tail>.*)\.sdb"
        m = re.match(serialpat, basename)
        if m:
            if not m.group('num') or not m.group('rev'):
                log("Problem getting serial from %s" % basename)
            serial = "%s-%s" % (m.group('num'), m.group('rev'))
            tail = m.group('tail')
        # some users have multiple sdb files that only differ
        # toward the end of their filename. Add that as well
        # to ensure unique temp table names
        if not tail:
            log("SDB tail empty. Wassup with that?")
        tableprefix = '_%s_%s_%s' % (iidstr, serial, tail)
        # eliminate illegal chars in tableprefix
        tableprefix = tableprefix.translate({ord(i):None for i in '-./'})

    # we also need to know the wear dates for this file in order to
    # filter out extraneous records
    wear_start, wear_end = get_participant_wear_dates(iidstr, city, wave, serial) 
    if not wear_start or not wear_end:
        warn("No wear dates in linkage tbl for user %s city %s wave %s serial %s"%(iidstr, city, wave, serial))
    else:
        mention("Keeping records within wear dates: %s - %s."%(wear_start,wear_end))

    # Now ingest each of the required tables into the DB.
    success = True
    for main_tbl in selectionsql:
        tableid = '%s_%s' % (tableprefix, main_tbl)
        success = success and execute_copy_via_shell(filepath, 
                                                     city, wave, stream,
                                                     iidstr, serial, tableid,
                                                     wear_start, wear_end,
                                                     main_tbl,
                                                     selectionsql[main_tbl]) 
    return success


def ingest_user_files(iidstr, city,wave,stream, filepathlist):
    """
    Given an interact_id string and a list of filepaths, 
    perform basic acceptance tests on the data files and
    if they pass, load them into the database, tagged with the iid.
    """
    log("Ingesting files inventoried for user: %s" % iidstr)

    # Perform basic acceptability tests on the candidate files
    # and reject any files that do not meet standards
    ingest_file_stats = select_usable_files(filepathlist, iidstr)

    if not ingest_file_stats:
        log("Files found for participant %s were not usable."%iidstr)
        return False

    log("User %s has ingestible files."%iidstr)

    # now actually ingest the datafiles
    success = True
    for [min_dt, max_dt, count, sdbfile] in ingest_file_stats:
        result = ingest_sdb_file(sdbfile,city,wave,stream,iidstr)
        success = success and result

    return success


def count_db_rows(tablename):
    """
    Given the name of a table in the Postgres DB, return 
    the row-count from that table.
    """
    # dbURI = 'postgresql://{}@{}:{}/{}'.format(db_user,
    #                                 db_host, db_host_port,
    #                                 db_name)
    # engine = create_engine(dbURI)
    # engine.execute(q)
    count=-1
    with psycopg2.connect(user=db_user,
                          host=db_host,
                          port=db_host_port,
                          database=db_name) as conn:
        c = conn.cursor()
        sql = "SELECT count(1) FROM %s.%s" % (db_schema,tablename)
        c.execute(sql)
        count = int(c.fetchone()[0])
    return count


def get_participant_wear_dates(iid,city,wave,serial):
    """Given user information, get wear dates from linkage table."""
    sql = """
            -- The protocol requires that the declared start date in 
            -- the linkage table be the day the participant actually 
            -- began wearing the device - not the day they received it.
            -- Also assumes the declared final day of wearing includes 
            -- any data up to 3am of the following day.
        SELECT  started_wearing as start, stopped_wearing::date + INTERVAL '27 hours' as end 
        FROM portal_dev.sensedoc_assignments
        WHERE interact_id = {iid}
        AND city_id = {city}
        AND wave_id = {wave}
        AND sensedoc_serial = {serial};
        """.format(serial=serial,city=city,wave=wave,iid=iid)
        # This WHERE clause removed now that ids are stored as integers
        # -- some serials in table have leading 0s and others do not
        # AND sensedoc_serial ~ '0*{serial}'
        # DOOFUS! Removing the 0-test was fine but removing that
        # phrase altogether from the WHERE clause created problems.
        # BAD PROGRAMMER! NO COOKIE FOR YOU!
    mention("Getting wear window with sql: %s"%sql)
    wear_start = ''
    wear_end = ''
    try:
        with psycopg2.connect(user=db_user,
                            host=db_host,
                            port=db_host_port,
                            database=db_name) as conn:
            c = conn.cursor()
            # log("Looking up participant wear dates with SQL:")
            # log(sql)
            c.execute(sql)
            row = c.fetchone()
            if row:
                wear_start = row[0]
                wear_end = row[1]
                # log("Result was:")
                # log(row)
            else:
                log("Unable to find wear dates for serial: '%s'"%serial)
    except Exception as e:
        err("SQL call failed: %s"%e)
    return wear_start, wear_end


if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    # if user has specified a log filename, use that instead 
    # of the default
    if args['-L']:
        logfile = args['-L']

    exceptions_csv = ''
    if args['-e']:
        exceptions_csv = args['-e']

    detect = args['-D']
    detect = True

    if args['-E']:
        print("Command line args:")
        pprint(args)
        print("DB_HOST: %s"%db_host)
        print("DB_HOST_PORT: %s"%db_host_port)
        print("DB_USER: %s"%db_user)
        print("DB_NAME: %s"%db_name)
        print("DB_SCHEMA: %s"%db_schema)
        print("Log file: %s"%logfile)
        print("leak detect: %s"%detect)
        exit()

    citynum = -1
    if args['CITYID']:
        citynum = int(args['CITYID'])
    wavenum = -1
    if args['WAVEID']:
        wavenum = int(args['WAVEID'])

    # This var shouldn't be needed anywhere, because this script only 
    # ingests sensedoc data, but I'll set it for completeness
    streamnum = 2

    if args['-H']:
        hygiene_dir = args['-H']
        if not os.path.isdir(hygiene_dir):
            print("Invalid hygiene directory: %s. Aborting."%hygiene_dir)
            exit()

    with open(logfile, 'w') as loghandle:
        log("Ingest script was run with following arguments:")
        log(args)

        # process ingest restrictions
        target_iids = []
        if args['-i']:
            target_iids.append(args['-i'])
            print("WARNING: Ingesting only IIDs " + ','.join(target_iids))

        # load the list of files we should explicitly skip
        exceptionfiles = []
        if exceptions_csv and os.path.isfile(exceptions_csv):
            with open(exceptions_fname, 'r') as fh:
                for row in csv.DictReader(filter(lambda row: row.strip()[0]!='#',fh)):
                    if row['action'] == 'skip':
                        exceptionfiles.append(row['filename'].strip())

        if args['PATH']:
            # Ingesting a single file is quick and easy.
            # (This is used primarily for testing.)
            if os.path.isfile(args['PATH']):
                fpath = args['PATH']
                if fpath.lower().endswith('.sdb'):
                    ingest_sdb_file( [fpath, "007"] )
                else:
                    err("Only .sdb files can be ingested.")
                exit()

        # Otherwise, we ingest directories recursively.
        rootdir = args['PATH']
        log("Root directory: %s" % rootdir)

        # The files we're looking for are organized in directories by
        # the unique pairing of interact_id and sdid. 
        # So there's no point in trying to ingest by directory, because
        # a single participant's SDBs can be found in multiple
        # directories. What makes more sense is to ingest all the 
        # specific sdb files associated with a particular user.
        participant_files = {}
        print("Taking file inventory...")
        for foldername in tqdm(os.listdir(rootdir)):
            folderpath = os.path.join(rootdir, foldername)
            if os.path.isdir(folderpath): # we have a data folder
                log("Inventory D %s" % folderpath)
                sdidstr = ''
                iidstr = foldername
                # sdb data for some participants split over mult folders
                if '_' in foldername:
                    iidstr,dummy,sdidstr = foldername.partition('_')
                # If we're limiting to a specific user, don't
                # both processing other people
                if target_iids and iidstr not in target_iids:
                    mention("Ignoring user %s who is not target."%iidstr)
                    continue
                # all sdb data within folder belongs to given participant
                for filename in os.listdir(folderpath):
                    if filename in exceptionfiles:
                        log("File '%s' is a known exception.  Skipping."%filename) 
                        continue
                    filepath = os.path.join(folderpath, filename)
                    if filename.lower().endswith('.sdb'):
                        if not iidstr in participant_files:
                            participant_files[iidstr] = []
                        participant_files[iidstr].append(filepath)
            else:
                log("Inventory F %s" % folderpath)

        # Now we have a dictionary with a list of file paths for each
        # participant

        # Before we ingest the files, take a row count from the existing
        # DB tables so that we can count again post-ingest and compare
        # against the number of rows we intended to ingest from each
        # of the SDB files. This will give some measure of confidence
        # as to whether the ingest was successful.
        # In this pre-ingest count, we report both GPS and Accel counts,
        # despite the higher cost of counting accel rows, so we could
        # conceivably use those record counts to rollback if the ingest
        # fails.
        baseline_gps_rows = count_db_rows('sd_gps')
        baseline_accel_rows = count_db_rows('sd_accel')
        log("Baseline GPS Count: %d"%baseline_gps_rows)
        log("Baseline Accel Count: %d"%baseline_accel_rows)

        # Now ingest all the files associated with each participant
        print("Ingesting inventoried files...")
        for iidstr in tqdm(participant_files):
            if participant_files[iidstr]:
                log("Inventory for %s holds: %s" % (iidstr,participant_files[iidstr]), prebreak=True)
                result = ingest_user_files(iidstr,
                                           citynum,wavenum,streamnum,
                                           participant_files[iidstr])
            else:
                log("Inventory ? %s" % iidstr)

        # Count final table sizes
        final_gps_rows = count_db_rows('sd_gps') 
        final_accel_rows = count_db_rows('sd_accel') 

        # Report before, after, and expected row counts. After the
        # initial baseline, we only track GPS table rows, as they are 
        # much smaller than the accel tables, so they're much faster to 
        # compute and still provide a reasonable metric of success.
        log("Baseline GPS Count: %d"%baseline_gps_rows)
        log("Expected GPS Count: %d"%(baseline_gps_rows+expected_gps_rows))
        log("Complete GPS Count: %d"%final_gps_rows)

        ingested_gps_rows = final_gps_rows - baseline_gps_rows
        ingest_rate = 100.0*float(ingested_gps_rows)/float(expected_gps_rows)
        if ingest_rate > 99.5:
            log("Based on GPS data, ingest looks successful with %0.2f%% row retention."%ingest_rate)
            log("Remember, some minor row loss is expected due to timestamp duplication in SDB files.")
        else:
            log("Row retention of %0.2f%% looks low."%ingest_rate) 
            log("Minor row loss is expected due to duplicated timestamps in the SDB files, but this seems excessive.") 
            log("But if this was a re-ingest of an uncompleted previous ingest cycle, high row loss is to be expected.")


        if warnings_issued:
            print("\nBE ADVISED THAT WARNINGS WERE ISSUED. (Check log for details.)")
        else:
            print("\nThe ingest was processed without warnings.")
