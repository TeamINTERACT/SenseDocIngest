#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script compares two directory trees to verify whether they contain
identical data content, despite having potentially different hierarchies and filenames.

This is done by creating two checksum fingerprint lists: one for all the files
in each tree. We then compare the two lists to see that they contain identical
sets of fingerprints.

Usage:
  verify_refactor [options] ROOTPATH1 ROOTPATH2
  verify_refactor -h | --help | -V | --version

Options:
    -h            Display this help info
    -v,--verbose  Provide more verbose output
"""

import os
import hashlib
from tqdm import tqdm
from docopt import docopt

def mention(str):
    """
    A 'verbosity-aware' print statement.
    """
    if args['--verbose']:
        print(str)

def compute_checksum(fpath):
    """
    For a given absolute filepath, compute the md5 checksum fingerprint.
    """
    md5 = hashlib.md5()
    mention("Verifying %s" % fpath)
    with open(fpath, "rb") as fh:
        for block in iter(lambda: fh.read(4096), b""):
            md5.update(block)

    return(md5.hexdigest())

def list_all_filepaths(rootdir):
    """
    Return a list of all filepaths rooted at rootdir in absolute form.
    """
    fpaths = []
    for root,dirs,files in os.walk(rootdir):
        for f in files:
            fpaths.append( os.path.join(root,f) )
    return(sorted(fpaths, key=os.path.basename))

def checksum_compare_filesets(fpaths1, fpaths2):
    """
    The challenge is to find a way to compare two sets of files for
    identicality without being able to rely on the file names and paths.
    So to do this, we'll traverse one entire pathlist and build a list 
    with checksums for each. Then we'll traverse the second, compute a
    checksum for each of those and remove them from the list.

    If we compute a checksum for the second group that does not exist in
    the checksums of the first group, then it's either already been removed (the file got duplicated) or the file got damaged (hence the reorged one has a different checksum) or it somehow got deleted from the
    reorged hierarchy.

    If the list of final checksums is empty, then everything is good.
    """
    checksum_list = {}
    unsourced_list = {}
    with tqdm(total=len(fpaths1)+len(fpaths2)) as pbar:
        for fpath in fpaths1:
            checksum_list[compute_checksum(fpath)] = fpath
            pbar.update(1)

        for fpath in fpaths2:
            cs = compute_checksum(fpath)
            if cs in checksum_list:
                del checksum_list[cs]
                # checksum_list.remove( compute_checksum(fpath) )
            else:
                unsourced_list[cs] = fpath
            pbar.update(1)

    if unsourced_list:
        print("The following dest files have no origin in the src tree:")
        for cs in unsourced_list:
            print("  %s  %s"%(cs, unsourced_list[cs]))

    if checksum_list:
        print("The following src files had no match in the dest tree:")
        for cs in checksum_list:
            print("  %s  %s"%(cs, checksum_list[cs]))
    else:
        print("Good news: All checksums match.")


if __name__ == '__main__':
    args = docopt(__doc__, version='0.1.1')

    rootpath1 = args['ROOTPATH1']
    rootpath2 = args['ROOTPATH2']

    fpaths1 = list_all_filepaths(rootpath1)
    fpaths2 = list_all_filepaths(rootpath2)

    if len(fpaths1) == len(fpaths2):
        print("Good news. Both trees contain %d data files."%len(fpaths1))
    else:
        print("Bad news. DIR1 contains %d files while DIR2 has %d"%(len(fpaths1),len(fpaths2)))

    checksum_compare_filesets(fpaths1, fpaths2)
